{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME TO THE ORCA NOTEBOOK\n",
    "\n",
    "Pipeline for the **O**bjective **R**elational **C**omparative **A**nalyses of mass spectral data, along with other data sources. All you need is a directory of mzXML files or a features vs samples bucket table to get started!\n",
    "\n",
    "To run cells of code, select cell and then press **Shift + Enter**. The first cell loads-in python modules necessary for the rest of the code to function\n",
    "\n",
    "In some of the cells below, the user will need to input information, such as setting paths, setting parameters, etc. In these cases, the user will see a cell with variables to be set at the top, followed by a line of '###', below which the rest of the code can be seen. Please set all applicable variables above the '###' line. Tinkering with the code below the '###' line is highly encouraged, as that is precisely why we chose to make it available as a Jupyter Notebook, however it could result in a 'breaking' of ORCA. If that appears to be the case, simply clone again from GitHub to get back to working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyteomics import mzxml, auxiliary\n",
    "import glob\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from statistics import mean\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from collections import Counter\n",
    "import sklearn.feature_selection\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=4, edgeitems=5, linewidth=100, suppress=True, threshold=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS1 Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains the first variable that you need to assign, if you are using ORCA to do MS1 feature processing - the path to the folder (directory) that contains your samples. If this notebook is in the same folder as your samples, the path should be './'. After you set the path, run the cell (**Shift + Enter**) and it should display a list of your mzXML sample files. If your files are not displayed, your path may be wrong.\n",
    "\n",
    "Note: If you already have a features vs samples bucket table, and are not trying to use ORCA for MS1 Feature Processing, please proceed to the section in the notebook titled **Analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample_directory = './' \n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "#Load files from the indicated directory with .mzXML extension\n",
    "fns = glob.glob(path_to_sample_directory +'*.mzXML')\n",
    "fns.sort()\n",
    "peak_list = []\n",
    "peak_range_low = []\n",
    "peak_range_high = []\n",
    "rt_list = []\n",
    "rt_range_low = []\n",
    "rt_range_high = []\n",
    "rrt_list = []\n",
    "rrt_range_low = []\n",
    "rrt_range_high = []\n",
    "integ = []\n",
    "sample = []\n",
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata for mzXML files\n",
    "This cell contains another variable for you to assign - the path to a metadata file, that has information about your samples. At minimum, this file needs a column titled 'filename' that includes the filename (including the .mzXML extension) for each of your samples. If this notebook is in the same folder as your metadata file, the path should be './*name-of-metadata-file.csv*'. After you set the path, run the cell and it should display your metadata file as a table, with two additional columns added that include modified versions of the filenames - these are important for mapping metadata later on in the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_metadata = './metadata_laulauamide.csv'\n",
    "\n",
    "##############################################################################################################\n",
    "#load in metadata file, mapping file names to other information\n",
    "\n",
    "md = pd.read_csv(path_to_metadata)\n",
    "md['filename_stripped'] = [f.split('.mzXML')[0] for f in md['filename']]\n",
    "md['filename_path'] = [path_to_sample_directory + f for f in md['filename']]\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom retention time function\n",
    "The original intent behind the design of ORCA was to be able to perform MS1 analyses on heterogeneous datasets (e.g. comparing data files acquired at different time points, under different chromatographic conditions, etc). In particular, we were faced with comparing current samples and legacy samples that had been run on similar but not the same chromatgraphic method (namely, a 2-min increase in run length to compensate for a XX decrease in flow-rate). This meant that using retention time as a second dimension (along with m/z) to charcaterize MS1 features did not allow of the proper consolidation of MS1 features across different samples. We were able to overcome this by instead using relative retention, along with applying an appropriate relative retention time tolerance window as determined through manual validation.\n",
    "\n",
    "Datasets that are more chromatographically hterogeneous than ours will likely require more complex functions applied to retention time in order for it to remain a reasonable and reliable second dimension by which MS1 features can be consolidated. We provide the opportunity to apply custom retention time functions in the cell below. Some ideas are: XX\n",
    "\n",
    "As is always the case when setting parameters or applying custom functions in tools, it is the user's responsibility to manually validate that appropriate parameters and functions have been applied. What this specifically means is checking that features known to be the same across different samples are in fact being considered the same in the resultant features vs samples bucket table. If this is not the case, please change the parameters/functions. The best guidance on selecting appropriate parameters and functions comes from studying and identifying the nuances in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom retention time function\n",
    "\n",
    "def custom_rt(rt, total_run_time):\n",
    "    return rt/total_run_time\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paramters for MS1 feature picking\n",
    "\n",
    "Here are descriptions and recommendations for the different parameters to be set\n",
    "\n",
    "**bin_width**: The first step in generating feature lists for each sample is to divide the range of m/z values to be considered into bins, sorting features into these bins based on m/z, and then grouping peaks within each bin based on those peaks occurring in consecutive MS1 scans. Bin width determines the size of these bins. Appropriately setting bin width depends on the resolution of the instrument(s) from which data was acquired, and is essentially a form or m/z tolerance. For low resolution instruments/heterogenous sample sets, 1 is a safe starting point.\n",
    "\n",
    "**bin_offset**: This parameter is unnecessary for most uses, and so should defaultly be set to 0. It is useful in cases where a systematic bias in m/z is seen in the data, e.g. if a feature that should have an m/z of 100.0 is consistently being recorded as 100.3, a bin_offset of 0.3 may be appropriate.\n",
    "\n",
    "**bins_start**: The lowest m/z to be considered.\n",
    "\n",
    "**bins_end**: The highest m/z to be considered.\n",
    "\n",
    "**peak_consecutivity**: This parameter designates how 'consecutive' peaks need to be in order to be considered as composing the same feature. A value of 0 means that peaks will only be considered as part of a singular feature if there are 0 MS1 scans between them in which the peak does not appear. A value of 1 means that if a peak does not appear in one MS1 scan, but it does appear in the scans directly before and after, those instances can will still be grouped together as one feature. For guidedance in setting this parameter, take a look at your data - do peaks tend to occur in consecutive MS1 scans, or are there sometimes gaps?\n",
    "\n",
    "**peak_cluster_size_cutoff**: This parameter designates how wide (e.g. how many consecutive scans it is found across) a potential feature needs to be in order to be considered a true feature. For example, setting this parameter to 3 means that a potential feature must be recorded in three or more scans meeting the consecutivity criyteria designated via the peak_consecutivity parameter in order to be considered a true feature.\n",
    "\n",
    "**min_integral**: Minimum integral a potential feature must have in order to be considered a true feature.\n",
    "\n",
    "**rt_setting**: This parameter determines what is used as the second dimension, along with m/z, is used for defining specific features. The options are 'raw' for using regular retention time, 'relative' for using relative retention time, or 'custom' for applying a custom retention time function that the user can define above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter MS1 feature picking paramaters:\n",
    "bin_width = 1\n",
    "bin_offset = 0 \n",
    "bins_start = 200\n",
    "bins_end = 2000\n",
    "peak_consecutivity = 0\n",
    "peak_cluster_size_cutoff = 3\n",
    "min_integral = 100000\n",
    "\n",
    "rt_setting = 'relative'\n",
    "\n",
    "##########################################################################################################################\n",
    "#From each file, bin and integrate peaks from all MS1 scans\n",
    "if rt_setting not in ['relative', 'raw', 'custom']:\n",
    "    print('rt_setting is not properly set. The options are \"raw\", \"relative\", or \"custom\". Please try again.')\n",
    "    raise SystemExit(0)\n",
    "for file_name in fns:\n",
    "    print(file_name)\n",
    "    rt = []\n",
    "    run_length = []\n",
    "    mz = []\n",
    "    inten = []\n",
    "    with mzxml.read(file_name) as reader:\n",
    "        for spectrum in reader:\n",
    "            run_length.append([spectrum['retentionTime']])\n",
    "            if spectrum['msLevel'] == 1:\n",
    "                rt.extend([spectrum['retentionTime']] * len(spectrum['m/z array']))\n",
    "                mz.extend(spectrum['m/z array'])\n",
    "                inten.extend(spectrum['intensity array'])\n",
    "    run_length = np.max(run_length)            \n",
    "    rt = np.array(rt, dtype = np.float32)\n",
    "    rt_unique = np.unique(rt)\n",
    "    rt_rank = pd.factorize(rt)[0]\n",
    "    mz = np.array(mz, dtype = np.float32)\n",
    "    inten = np.array(inten, dtype = np.float32)\n",
    "    \n",
    "    bins = np.arange(bins_start, bins_end, bin_width)\n",
    "    for b in bins:\n",
    "        rt_positions = rt_rank[np.where((mz > (b - (bin_width/2) + bin_offset)) * (mz < (b + (bin_width/2) + bin_offset)))[0]]\n",
    "        rt_positions = np.array(list(set(rt_positions)))\n",
    "        #cluster rt_positions when they are consecutive\n",
    "        clusters = list(np.split(rt_positions, np.where(np.diff(rt_positions) > peak_consecutivity+1)[0]+1))\n",
    "        cluster_size = np.array([len(c) for c in clusters])\n",
    "        clusters_filt = [clusters[i] for i in list(np.where(cluster_size > peak_cluster_size_cutoff)[0])]\n",
    "        for y in clusters_filt:\n",
    "            idx = np.where((mz > (b - (bin_width/2) + bin_offset )) * (mz < (b + (bin_width/2) + bin_offset )) * (rt_rank >= np.min(y)) * (rt_rank <= np.max(y)))[0]\n",
    "            xcoord = rt[idx]\n",
    "            ycoord = inten[idx]\n",
    "            integral = np.trapz(y = ycoord, x = xcoord)\n",
    "            if integral > min_integral:\n",
    "                integ.append(integral)\n",
    "                peak_list.append(np.mean(mz[idx]))\n",
    "                peak_range_low.append(np.min(mz[idx]))\n",
    "                peak_range_high.append(np.max(mz[idx]))\n",
    "                rt_list.append(np.mean(rt[idx]))\n",
    "                rt_range_low.append(np.min(rt[idx]))\n",
    "                rt_range_high.append(np.max(rt[idx]))\n",
    "                sample.append(file_name)\n",
    "                if rt_setting == 'relative':\n",
    "                    rrt_list.append(np.mean(rt[idx])/run_length)\n",
    "                    rrt_range_low.append(np.min(rt[idx]/run_length))\n",
    "                    rrt_range_high.append(np.max(rt[idx]/run_length))\n",
    "                elif rt_setting == 'raw':\n",
    "                    rrt_list.append(np.mean(rt[idx]))\n",
    "                    rrt_range_low.append(np.min(rt[idx]))\n",
    "                    rrt_range_high.append(np.max(rt[idx]))\n",
    "                elif rt_setting == 'custom':\n",
    "                    rrt_list.append(custom_rt(np.mean(rt[idx]), run_length))\n",
    "                    rrt_range_low.append(custom_rt(np.min(rt[idx]), run_length))\n",
    "                    rrt_range_high.append(custom_rt(np.max(rt[idx]), run_length))\n",
    "                else:\n",
    "                    print('rt_setting is not properly set. The options are \"raw\", \"relative\", or \"custom\". Please try again.')\n",
    "                    raise SystemExit(0)\n",
    "#         print('current bin: ' + str(b))\n",
    "#         print('current length of sample: ' + str(len(sample)))\n",
    "#         print('current length of rrt_list: ' + str(len(rrt_list)))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paramters for MS1 feature consolidation\n",
    "\n",
    "Here are descriptions and recommendations for the different parameters to be set\n",
    "\n",
    "**rrt_tolerance**: This is the maximum raw/relative/custom retention time distance between the edges of two features in order for them to be candidates for consolidation.\n",
    "\n",
    "**bin_width**: Bin width, again, determines the size of m/z feature bins. Appropriately setting bin width depends on the resolution of the instrument(s) from which data was acquired, and is essentially a form or m/z tolerance. For low resolution instruments/heterogenous sample sets, 1 is a safe starting point.\n",
    "\n",
    "**bin_offset**: This parameter is unnecessary for most uses, and so should defaultly be set to 0. It is useful in cases where a systematic bias in m/z is seen in the data, e.g. if a feature that should have an m/z of 100.0 is consistently being recorded as 100.3, a bin_offset of 0.3 may be appropriate.\n",
    "\n",
    "**bins_start**: The lowest m/z to be considered.\n",
    "\n",
    "**bins_end**: The highest m/z to be considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add feature consolidation parameters\n",
    "rrt_tolerance = 0.05 #maximum rrt distance between edges of two feature peaks in order for them to be candidates for consolidation\n",
    "bin_width = 1\n",
    "bin_offset = 0 #must be less than half of bin_width\n",
    "bins_start = 200\n",
    "bins_end = 2000\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#Peak consoslidating\n",
    "\n",
    "masses = np.arange(bins_start, bins_end, bin_width)\n",
    "peak_list = np.array(peak_list, dtype = np.float32)\n",
    "rrt_list = np.array(rrt_list, dtype = np.float32)\n",
    "rrt_range_high = np.array(rrt_range_high, dtype = np.float32)\n",
    "rrt_range_low = np.array(rrt_range_low, dtype = np.float32)\n",
    "\n",
    "updated_integ = []\n",
    "updated_peak_list = []\n",
    "updated_rrt_list = []\n",
    "updated_rrt_range_low = []\n",
    "updated_rrt_range_high = []\n",
    "updated_sample = []\n",
    "temp_peaks = []\n",
    "\n",
    "#search through each mass, and collapse peaks across samples, based on rrt tolerance/overlap\n",
    "for mass in masses:\n",
    "    print(mass)\n",
    "    idx = list(np.where((peak_list > (mass - (bin_width/2) + bin_offset)) * (peak_list < (mass + (bin_width/2) + bin_offset)))[0])\n",
    "    i_record = []\n",
    "    for i in idx:\n",
    "        temp_peaks_flag = 0\n",
    "        for j in idx:\n",
    "            if j != i:\n",
    "                if sample[i] != sample[j] and max(rrt_range_low[i],rrt_range_low[j]) < min(rrt_range_high[i],rrt_range_high[j]) + rrt_tolerance:\n",
    "                    temp_peaks_flag += 1\n",
    "                    temp_peaks.append([i,j])\n",
    "\n",
    "        i_record.append(i)\n",
    "        if temp_peaks_flag == 0:\n",
    "            updated_integ.append(integ[i])\n",
    "            updated_peak_list.append(peak_list[i])\n",
    "            updated_rrt_list.append(rrt_list[i])\n",
    "            updated_rrt_range_low.append(rrt_range_low[i])\n",
    "            updated_rrt_range_high.append(rrt_range_high[i])\n",
    "            updated_sample.append(sample[i])\n",
    "            \n",
    "\n",
    "for tp in temp_peaks:\n",
    "    tp.sort()\n",
    "    \n",
    "temp_peaks.sort()\n",
    "temp_peaks = list(tp for tp,_ in itertools.groupby(temp_peaks))\n",
    "\n",
    "#collapse peaks by graphing overlaps\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(temp_peaks)\n",
    "apeaks = list(nx.connected_components(G))\n",
    "\n",
    "#update peak lists to include adjusted mz and rrt for collapsed peaks\n",
    "for apeak in apeaks:\n",
    "    apeak = list(apeak)\n",
    "    curr_integ = [integ[a] for a in apeak]\n",
    "    curr_peak_list = [peak_list[a] for a in apeak]\n",
    "    curr_rrt_list = [rrt_list[a] for a in apeak]\n",
    "    curr_rrt_range_low = [rrt_range_low[a] for a in apeak]\n",
    "    curr_rrt_range_high  = [rrt_range_high[a] for a in apeak]\n",
    "    curr_sample = [sample[a] for a in apeak]\n",
    "    for file_name in list(set(curr_sample)):\n",
    "        csidx = [i for i, j in enumerate(curr_sample) if j == file_name]\n",
    "        updated_integ.append(sum([curr_integ[c] for c in csidx]))\n",
    "        updated_peak_list.append(np.mean(curr_peak_list))\n",
    "        updated_rrt_list.append(np.mean(curr_rrt_list))\n",
    "        updated_rrt_range_low.append(np.mean(curr_rrt_range_low))\n",
    "        updated_rrt_range_high.append(np.mean(curr_rrt_range_high))\n",
    "        updated_sample.append(file_name)\n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bucket table\n",
    "This next cell organizes features into a features vs samples bucket table, to which analyses can then be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate empty Pandas dataframe, to be populated as a bucket table\n",
    "columns = fns\n",
    "index0 = list(set(list(zip(updated_peak_list, updated_rrt_list))))\n",
    "index0.sort()\n",
    "df_updated = pd.DataFrame(index=index0, columns=columns)\n",
    "df_updated = df_updated.fillna(0)\n",
    "\n",
    "#populate bucket table\n",
    "updated_sample = np.array(updated_sample)\n",
    "updated_integ = np.array(updated_integ)\n",
    "for mz_rrt in index0:\n",
    "    for filename in fns:\n",
    "        idx_to_df = np.where((updated_peak_list == mz_rrt[0]) * (updated_rrt_list == mz_rrt[1]) * (updated_sample == filename))[0]\n",
    "        if len(idx_to_df) == 1:\n",
    "            value = updated_integ[idx_to_df]\n",
    "            df_updated.at[mz_rrt, filename] = value\n",
    "\n",
    "df_updated.index = pd.MultiIndex.from_tuples(index0, names=['mz', 'rrt'])\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~ End of MS1 Feature Processing ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-in features vs samples bucket table\n",
    "\n",
    "If you already have a features vs samples bucket table, you can load it into ORCA to perform analyses here. Input the path to your metadata table in the cell below. It should be either in tsv or csv format. If you load-in your bucket table and see that it includes columns or rows that it should not, add those columns or rows to a list under the variable **drop_columns** or **drop_rows**, respectively.\n",
    "\n",
    "Note: All of the following code is setup to work on a bucket table that is arranged so that the features are represented by each row, and the samples are represented by each column. If your bucket table is arranged in the alternative fashion (rows = samples, columns = features), please set **transpose_buckettable** to True.\n",
    " \n",
    "**If you used the MS1 Feature Processing to generate a bucket table above, you can skip the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BC4_BC4_01_57041.mzXML</th>\n",
       "      <th>BB2_BB2_02_57066.mzXML</th>\n",
       "      <th>BB2_BB2_01_57065.mzXML</th>\n",
       "      <th>BC2_BC2_01_57047.mzXML</th>\n",
       "      <th>BB5_dil2x_BB5_02_57202.mzXML</th>\n",
       "      <th>BB5_dil2x_BB5_01_57201.mzXML</th>\n",
       "      <th>BC2_BC2_02_57048.mzXML</th>\n",
       "      <th>BB8_dil2x_BB8_01_57187.mzXML</th>\n",
       "      <th>BB4_BB4_01_57063.mzXML</th>\n",
       "      <th>BB4_BB4_02_57064.mzXML</th>\n",
       "      <th>...</th>\n",
       "      <th>QC_BF1_02_57110.mzXML</th>\n",
       "      <th>QC_BF1_01_57109.mzXML</th>\n",
       "      <th>QC_BG1_02_57134.mzXML</th>\n",
       "      <th>QC_BD1_02_57062.mzXML</th>\n",
       "      <th>BF9_BF9_01_57123.mzXML</th>\n",
       "      <th>BG8_BG8_01_57091.mzXML</th>\n",
       "      <th>BG4_BG4_01_57055.mzXML</th>\n",
       "      <th>BF9_BF9_02_57124.mzXML</th>\n",
       "      <th>BG4_BG4_02_57056.mzXML</th>\n",
       "      <th>BG8_BG8_02_57092.mzXML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.014527e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66279.0550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106750.6125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58140.4665</td>\n",
       "      <td>92484.0935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.976554e+04</td>\n",
       "      <td>9.447252e+06</td>\n",
       "      <td>61441.3630</td>\n",
       "      <td>7.008092e+04</td>\n",
       "      <td>405376.8440</td>\n",
       "      <td>1.433134e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.852199e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>398825.8925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>98251.2425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49089.0505</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.861419e+05</td>\n",
       "      <td>7.984661e+06</td>\n",
       "      <td>903141.8515</td>\n",
       "      <td>3.827560e+05</td>\n",
       "      <td>886281.3700</td>\n",
       "      <td>8.194722e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.962282e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.740813e+07</td>\n",
       "      <td>3.945981e+07</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.794219e+07</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.963542e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.399368e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.144581e+07</td>\n",
       "      <td>3.164850e+07</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.167716e+07</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.177776e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.871969e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169963.2760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>37484.7525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.086498e+05</td>\n",
       "      <td>1.428909e+07</td>\n",
       "      <td>685900.5630</td>\n",
       "      <td>7.758418e+06</td>\n",
       "      <td>685654.0805</td>\n",
       "      <td>1.441964e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15429</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.028735e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15430</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.719475e+04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.877670e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15431</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.094449e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15432</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500896e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.692412e+05</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.959228e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15433</th>\n",
       "      <td>4.599344e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.463317e+04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.830189e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15434 rows Ã— 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BC4_BC4_01_57041.mzXML  BB2_BB2_02_57066.mzXML  BB2_BB2_01_57065.mzXML  \\\n",
       "0                2.014527e+07                     0.0                     0.0   \n",
       "1                2.852199e+07                     0.0                     0.0   \n",
       "2                2.962282e+07                     0.0                     0.0   \n",
       "3                2.399368e+07                     0.0                     0.0   \n",
       "4                2.871969e+04                     0.0                     0.0   \n",
       "...                       ...                     ...                     ...   \n",
       "15429            0.000000e+00                     0.0                     0.0   \n",
       "15430            0.000000e+00                     0.0                     0.0   \n",
       "15431            0.000000e+00                     0.0                     0.0   \n",
       "15432            0.000000e+00                     0.0                     0.0   \n",
       "15433            4.599344e+04                     0.0                     0.0   \n",
       "\n",
       "       BC2_BC2_01_57047.mzXML  BB5_dil2x_BB5_02_57202.mzXML  \\\n",
       "0                  66279.0550                           0.0   \n",
       "1                 398825.8925                           0.0   \n",
       "2                      0.0000                           0.0   \n",
       "3                      0.0000                           0.0   \n",
       "4                 169963.2760                           0.0   \n",
       "...                       ...                           ...   \n",
       "15429                  0.0000                           0.0   \n",
       "15430                  0.0000                           0.0   \n",
       "15431                  0.0000                           0.0   \n",
       "15432                  0.0000                           0.0   \n",
       "15433                  0.0000                           0.0   \n",
       "\n",
       "       BB5_dil2x_BB5_01_57201.mzXML  BC2_BC2_02_57048.mzXML  \\\n",
       "0                       106750.6125                  0.0000   \n",
       "1                            0.0000              98251.2425   \n",
       "2                            0.0000                  0.0000   \n",
       "3                            0.0000                  0.0000   \n",
       "4                            0.0000              37484.7525   \n",
       "...                             ...                     ...   \n",
       "15429                        0.0000                  0.0000   \n",
       "15430                        0.0000                  0.0000   \n",
       "15431                        0.0000                  0.0000   \n",
       "15432                        0.0000                  0.0000   \n",
       "15433                        0.0000                  0.0000   \n",
       "\n",
       "       BB8_dil2x_BB8_01_57187.mzXML  BB4_BB4_01_57063.mzXML  \\\n",
       "0                               0.0              58140.4665   \n",
       "1                               0.0              49089.0505   \n",
       "2                               0.0                  0.0000   \n",
       "3                               0.0                  0.0000   \n",
       "4                               0.0                  0.0000   \n",
       "...                             ...                     ...   \n",
       "15429                           0.0                  0.0000   \n",
       "15430                           0.0                  0.0000   \n",
       "15431                           0.0                  0.0000   \n",
       "15432                           0.0                  0.0000   \n",
       "15433                           0.0                  0.0000   \n",
       "\n",
       "       BB4_BB4_02_57064.mzXML  ...  QC_BF1_02_57110.mzXML  \\\n",
       "0                  92484.0935  ...                    0.0   \n",
       "1                      0.0000  ...                    0.0   \n",
       "2                      0.0000  ...                    0.0   \n",
       "3                      0.0000  ...                    0.0   \n",
       "4                      0.0000  ...                    0.0   \n",
       "...                       ...  ...                    ...   \n",
       "15429                  0.0000  ...                    0.0   \n",
       "15430                  0.0000  ...                    0.0   \n",
       "15431                  0.0000  ...                    0.0   \n",
       "15432                  0.0000  ...                    0.0   \n",
       "15433                  0.0000  ...                    0.0   \n",
       "\n",
       "       QC_BF1_01_57109.mzXML  QC_BG1_02_57134.mzXML  QC_BD1_02_57062.mzXML  \\\n",
       "0                        0.0                    0.0                    0.0   \n",
       "1                        0.0                    0.0                    0.0   \n",
       "2                        0.0                    0.0                    0.0   \n",
       "3                        0.0                    0.0                    0.0   \n",
       "4                        0.0                    0.0                    0.0   \n",
       "...                      ...                    ...                    ...   \n",
       "15429                    0.0                    0.0                    0.0   \n",
       "15430                    0.0                    0.0                    0.0   \n",
       "15431                    0.0                    0.0                    0.0   \n",
       "15432                    0.0                    0.0                    0.0   \n",
       "15433                    0.0                    0.0                    0.0   \n",
       "\n",
       "       BF9_BF9_01_57123.mzXML  BG8_BG8_01_57091.mzXML  BG4_BG4_01_57055.mzXML  \\\n",
       "0                7.976554e+04            9.447252e+06              61441.3630   \n",
       "1                3.861419e+05            7.984661e+06             903141.8515   \n",
       "2                2.740813e+07            3.945981e+07                  0.0000   \n",
       "3                2.144581e+07            3.164850e+07                  0.0000   \n",
       "4                1.086498e+05            1.428909e+07             685900.5630   \n",
       "...                       ...                     ...                     ...   \n",
       "15429            0.000000e+00            0.000000e+00                  0.0000   \n",
       "15430            0.000000e+00            5.719475e+04                  0.0000   \n",
       "15431            0.000000e+00            0.000000e+00                  0.0000   \n",
       "15432            1.500896e+05            0.000000e+00                  0.0000   \n",
       "15433            0.000000e+00            7.463317e+04                  0.0000   \n",
       "\n",
       "       BF9_BF9_02_57124.mzXML  BG4_BG4_02_57056.mzXML  BG8_BG8_02_57092.mzXML  \n",
       "0                7.008092e+04             405376.8440            1.433134e+05  \n",
       "1                3.827560e+05             886281.3700            8.194722e+06  \n",
       "2                2.794219e+07                  0.0000            3.963542e+07  \n",
       "3                2.167716e+07                  0.0000            3.177776e+07  \n",
       "4                7.758418e+06             685654.0805            1.441964e+07  \n",
       "...                       ...                     ...                     ...  \n",
       "15429            0.000000e+00                  0.0000            3.028735e+04  \n",
       "15430            0.000000e+00                  0.0000            4.877670e+04  \n",
       "15431            0.000000e+00                  0.0000            1.094449e+05  \n",
       "15432            1.692412e+05                  0.0000            2.959228e+04  \n",
       "15433            0.000000e+00                  0.0000            3.830189e+04  \n",
       "\n",
       "[15434 rows x 156 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckettable_path = '../../MZmine-2.53-Windows/Mb_MS_0_quant.csv'\n",
    "drop_columns = ['Unnamed: 159', 'row ID', 'row m/z', 'row retention time']\n",
    "col_ref = True\n",
    "drop_rows = []\n",
    "row_ref = False\n",
    "sample_name_trim = ' Peak area'\n",
    "\n",
    "transpose_buckettable = False\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if buckettable_path.split('.')[-1] == 'tsv':\n",
    "    bt = pd.read_table(buckettable_path)\n",
    "elif buckettable_path.split('.')[-1] == 'csv':\n",
    "    bt = pd.read_csv(buckettable_path)\n",
    "else:\n",
    "    print('Format invalid. File must be csv or tsv.')\n",
    "    \n",
    "if len(drop_columns) > 0:\n",
    "    if col_ref == True:\n",
    "        col_ref_df = bt[drop_columns]\n",
    "    bt.drop(drop_columns, axis = 1, inplace=True)\n",
    "\n",
    "if len(drop_rows) > 0:\n",
    "    if row_ref == True:\n",
    "        row_ref_df = bt.iloc[drop_rows]\n",
    "    bt.drop(drop_rows, axis = 0, inplace=True)\n",
    "\n",
    "if transpose_buckettable == True:\n",
    "    bt = bt.T\n",
    "    \n",
    "bt.columns = bt.columns.str.replace(sample_name_trim, '')\n",
    "\n",
    "df_updated = bt.copy(deep=True)\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in buckettable, normalize to internal reference, and re-export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckettable_path = '../../MZmine-2.53-Windows/Mb_MS_0_gnpsfilt_quant.csv'\n",
    "drop_columns = ['Unnamed: 159', 'row ID', 'row m/z', 'row retention time']\n",
    "col_ref = True\n",
    "drop_rows = []\n",
    "row_ref = False\n",
    "sample_name_trim = ''\n",
    "\n",
    "transpose_buckettable = False\n",
    "\n",
    "IS_ref_column = 'row ID'\n",
    "IS_ref_value = 1454\n",
    "\n",
    "export = True\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if buckettable_path.split('.')[-1] == 'tsv':\n",
    "    bt = pd.read_table(buckettable_path)\n",
    "elif buckettable_path.split('.')[-1] == 'csv':\n",
    "    bt = pd.read_csv(buckettable_path)\n",
    "else:\n",
    "    print('Format invalid. File must be csv or tsv.')\n",
    "    \n",
    "IS_index = bt[bt[IS_ref_column] == IS_ref_value].index\n",
    "\n",
    "if len(drop_columns) > 0:\n",
    "    if col_ref == True:\n",
    "        col_ref_df = bt[drop_columns]\n",
    "    bt.drop(drop_columns, axis = 1, inplace=True)\n",
    "\n",
    "if len(drop_rows) > 0:\n",
    "    if row_ref == True:\n",
    "        row_ref_df = bt.iloc[drop_rows]\n",
    "    bt.drop(drop_rows, axis = 0, inplace=True)\n",
    "    \n",
    "if transpose_buckettable == True:\n",
    "    bt = bt.T\n",
    "    \n",
    "bt.columns = bt.columns.str.replace(sample_name_trim, '')\n",
    "\n",
    "replace_val = bt.loc[IS_index].T.mean().values[0]\n",
    "bt.loc[IS_index] = bt.loc[IS_index].replace(0,replace_val)\n",
    "bt = bt/np.array(bt.loc[IS_index])\n",
    "\n",
    "## NEED TO FIGURE OUT HOW TO BEST ORGANIZE THIS\n",
    "df_IS_normed = pd.concat([col_ref_df, bt], axis=1).drop(['Unnamed: 159'], axis = 1)\n",
    "df_IS_normed.to_csv('../../MZmine-2.53-Windows/Mb_MS_0_gnpsfilt_quant_ISnormed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata for features vs samples bucket table\n",
    "This cell contains another variable for you to assign - the path to a metadata file, that has information about the samples in your bucket table. At minimum, this file needs a column with the names of your samples, matching how they are named in the bucket table. This column will be used to map any other metadata to the samples. \n",
    "\n",
    "**If you used the MS1 Feature Processing to generate a bucket table above, and you have already loaded-in a metadata table above, you can skip the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_metadata = '../../ReDU_v3.0_Mb_extracts_draft.tsv'\n",
    "\n",
    "#############################################################################\n",
    "#load in metadata file, mapping file names to other information\n",
    "if path_to_metadata.split('.')[-1] == 'tsv':\n",
    "    md = pd.read_table(path_to_metadata)\n",
    "elif path_to_metadata.split('.')[-1] == 'csv':\n",
    "    md = pd.read_csv(path_to_metadata)\n",
    "\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom transformation for your data\n",
    "Below, a number of transformations can be applied to your bucket table. If you cannot find a transformation that meets your needs, feel free to create a custom transformation in the cell directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformation\n",
    "def custom_transform(df_transformed):\n",
    "    #Edit here#\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "Options for transformations to be applied to your bucket table are log (base 10) and natural log transformations, unit vector normalization, conversion of feature values to proportions per sample, conversion of feature values to presence-absence data, or any custom transformation defined by the user. In order to apply a transformation, set the applicable variable to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "log_10_transform = True\n",
    "log_natural_transform = False\n",
    "\n",
    "unit_vector_normalization = False\n",
    "transform_to_proportions = False\n",
    "presence_absence = False\n",
    "\n",
    "custom_transform = False\n",
    "\n",
    "#############################################################################\n",
    "df_transformed = df_updated.copy(deep = True)\n",
    "flag_transf = 0\n",
    "\n",
    "if log_10_transform == True and log_natural_transform == True:\n",
    "    flag_transf = 1\n",
    "    print('Cannot use both log transforms at the same time.')\n",
    "\n",
    "#log (base 10) transform integrals\n",
    "if log_10_transform == True and flag_transf == 0:\n",
    "    df_transformed[df_transformed < 100] = 1\n",
    "    df_transformed = np.log10(df_transformed)\n",
    "    \n",
    "#log (natural) transform integrals\n",
    "if log_natural_transform == True and flag_transf == 0:\n",
    "    df_transformed[df_transformed < 100] = 1\n",
    "    df_transformed = np.log10(df_transformed)\n",
    "    \n",
    "# Unit vector normalization\n",
    "if unit_vector_normalization == True:\n",
    "    df_transformed = df_transformed/(df_transformed**2).sum()**0.5\n",
    "    \n",
    "# Change integrals to proportions\n",
    "if transform_to_proportions == True:\n",
    "    df_transformed = df_transformed/df_transformed.sum()\n",
    "    \n",
    "# Change presence-absence (1-0) data\n",
    "if presence_absence == True:\n",
    "    df_transformed[df_transformed > 0] = 1\n",
    "    \n",
    "    \n",
    "#Applies custom transform\n",
    "if custom_transform == True:\n",
    "    df_transformed = custom(df_transformed)\n",
    "    \n",
    "df_transformed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Set Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features from blanks [indicate process, solvent, qc_mix, and internal_standard seperately  **Order does not matter]\n",
    "filterby = 'BlankType'\n",
    "label_to_drop = ['process','solvent','qc_mix','internal_standard']\n",
    "filename_column = 'filename'\n",
    "\n",
    "################################################################################################\n",
    "for blank in label_to_drop:\n",
    "    query = filterby + ' == \"' + blank + '\"'\n",
    "    print(blank)\n",
    "    print(len(df_transformed[md.query(query)['filename']][df_transformed[md.query(query)['filename']].T.median() > 0]))\n",
    "    df_transformed.drop(df_transformed[md.query(query)['filename']][df_transformed[md.query(query)['filename']].T.median() > 0].index, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blanks \n",
    "filterby = 'SampleType'\n",
    "label_to_drop = 'blank_QC'\n",
    "filename_column = 'filename'\n",
    "\n",
    "################################################################################################\n",
    "query = filterby + ' != \"' + label_to_drop + '\"'\n",
    "df_transformed = df_transformed[md.query(query)['filename']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non Mb (Panama, unknown, Puerto Rico)\n",
    "filterby = 'ATTRIBUTE_collection_region'\n",
    "label_to_drop = 'Panama'\n",
    "filename_column = 'filename'\n",
    "\n",
    "################################################################################################\n",
    "query = filterby + ' == \"' + label_to_drop + '\"'\n",
    "df_transformed.drop(list(md.query(query)['filename']), axis = 1, inplace=True)\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_transformed['BF11_dil2x_BF11_01_57203.mzXML'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_transformed['BF11_dil2x_BF11_02_57204.mzXML'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove peaks not present in both reps (remove blanks first!!)\n",
    "cols = list(df_transformed.columns)\n",
    "samps = list(set([c.split('_0')[0] for c in cols]))\n",
    "for s in samps:\n",
    "    print(s)\n",
    "    for i in df_transformed.index:\n",
    "        if sum(df_transformed[df_transformed.filter(like=s).columns].loc[i] > 0) != 2:\n",
    "            df_transformed.at[i, df_transformed.filter(like=s).columns[0]] = 0\n",
    "            df_transformed.at[i, df_transformed.filter(like=s).columns[1]] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_transformed.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_0.copy(deep=True)\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dups\n",
    "df_transformed = df_transformed[df_transformed.filter(like='_01').columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop COMPLETELY empty features\n",
    "df_transformed.drop(df_transformed[df_transformed.T.sum() < 0.00000000001].index, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # peak cutoff\n",
    "# df_transformed = df_transformed.apply(lambda x: np.where(x < 6.5,0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop empty features\n",
    "# df_transformed.drop(df_transformed[df_transformed.T.sum() < 0.1].index, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring features, esp. singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed.T > 0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_transformed[np.sum(df_transformed.T > 0) == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ref_df.loc[15416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to see if feat (esp. singletons) was aligned optimally\n",
    "# via compare it to other feats with similar m/z\n",
    "\n",
    "min_mz = 457\n",
    "max_mz = 458\n",
    "#######################################\n",
    "\n",
    "col_ref_df[col_ref_df['row m/z'] > min_mz][col_ref_df[col_ref_df['row m/z'] > min_mz]['row m/z'] < max_mz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_transformed.loc[5]['BE6_dil2x_BE6_02_57212.mzXML']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit vector transform prior to PCA\n",
    "df_transformed = df_transformed/(df_transformed**2).sum()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =df_transformed.T\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, index = df_transformed.T.index, columns = ['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10'])\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels = 'ATTRIBUTE_shrimp_v_nonshrimp'\n",
    "ref_for_labels = 'filename' #should be the column whose contents exactly match the sample names in df_transformed\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "samplenames = [dict_labels[n] for n in df_transformed]\n",
    "samplenames\n",
    "\n",
    "principalDf['labels'] = samplenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf\n",
    "groups = principalDf.groupby('labels')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.pc1, group.pc2, marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel('PC1 [9.7%]')\n",
    "ax.set_ylabel('PC2 [8.4%]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, values = zip(*Counter(np.sum(df_transformed.T > 0)).most_common())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.title('Feature Frequency Distribution', fontsize=20)\n",
    "plt.ylabel('# of features', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('# of samples in which feature is detected', fontsize=18)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(np.sum(df_transformed.T > 0)).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed.T > 0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ref_df.loc[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ref_df.loc[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.sum(df_transformed > 0).sort_values(ascending = False).index\n",
    "values = np.sum(df_transformed > 0).sort_values(ascending = False).values\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.title('Features per sample', fontsize=20)\n",
    "plt.ylabel('# of features', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('samples', fontsize=18)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = np.sum(df_transformed > 0)\n",
    "max_peaks = np.max(df_transformed)\n",
    "pd.concat([num_feats, max_peaks], axis=1).plot.scatter(x=0,y=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)\n",
    "\n",
    "num_feats = np.sum(df_transformed > 0)\n",
    "num_sungletons = np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)\n",
    "pd.concat([num_feats, num_sungletons], axis=1).plot.scatter(x=0,y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed > 0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_0.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_0_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(df_transformed.apply(lambda x: np.where(x < 6,0,x)) > 0).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_0.copy(deep=True)\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features based on frequency across samples\n",
    "freq = [49,48]\n",
    "for f in freq:\n",
    "    df_transformed.drop(df_transformed[np.sum(df_transformed.T > 0) == f].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all features not in the top k peaks for a sample\n",
    "k = 250\n",
    "\n",
    "index_to_keep = []\n",
    "\n",
    "for c in df_transformed.columns:\n",
    "    cutoff = df_transformed[c].sort_values(ascending=False).iloc[k]\n",
    "    index_to_keep.extend(list(df_transformed[df_transformed[c] > cutoff].index))\n",
    "\n",
    "index_to_keep = list(set(index_to_keep))\n",
    "df_transformed = df_transformed.loc[index_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all peaks not in top k\n",
    "k = 20\n",
    "\n",
    "for c in df_transformed.columns:\n",
    "    print(np.sum(df_transformed[c] > 0))\n",
    "    cutoff = df_transformed[c].sort_values(ascending=False).iloc[k]\n",
    "    df_transformed[c] = df_transformed[c].apply(lambda x: np.where(x <= cutoff,0,x))\n",
    "    print(np.sum(df_transformed[c] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features not in top k\n",
    "k = 150\n",
    "df_transformed = df_transformed[np.sum(df_transformed.T) > np.sum(df_transformed.T).sort_values(ascending=False).iloc[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering + Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering + Dendrogram\n",
    "Hierarchical clustering will be performed on the bucket table generated via ORCA MS1 feature processing or on the bucket table that was independently loaded-in - whichever happened most recently.\n",
    "\n",
    "Here are the variables that must be set:\n",
    "\n",
    "**title**: The title that you would like to assign to the dendrogram.\n",
    "\n",
    "**sample_labels**: The name of the column from the metadata table that includes the names/identifiers of your samples that you would like to use in labelling the branches of the dendrogram.\n",
    "\n",
    "**ref_for_labels**: The name of the column for the metadata table that includes the names/identifiers of your samples as written in the bucket table. It is important that this variable is assigned correctly, as it allows metadata to be mapped properly to your samples.\n",
    "\n",
    "**metric**: The distance metric used in performing the hierarchical clustering. For more information on options for distance metrics, check out the documentation for SciPy, specifically the pdist function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\n",
    "\n",
    "**method**: The clustering method used in performing the hierarchical clustering. For more information on options for distance metrics, check out the documentation for SciPy, specifically the linkage function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n",
    "\n",
    "**color_cutoff**: The distance value below which you wish clusters to be colorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = 'Moorena bouillonii Chemogeography - Guam, core feats. by subregion [cutoff = 0.50]'\n",
    "sample_labels = 'ATTRIBUTE_collection_subregion'\n",
    "ref_for_labels = 'filename' #should be the column whose contents exactly match the sample names in df_transformed\n",
    "metric = 'cosine'\n",
    "method = 'average'\n",
    "color_cutoff = 0.7\n",
    "dataframe = cores #usually df_transformed\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "samplenames = [dict_labels[n] for n in dataframe]\n",
    "\n",
    "#Clustering linkages followed by dendrogram construction\n",
    "matplotlib.rcParams['lines.linewidth'] = 3\n",
    "hc = linkage(dataframe.T, method, metric = metric)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(title, fontsize=20)\n",
    "plt.ylabel(metric + ' distance', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "dendrogram(\n",
    "    hc,\n",
    "    color_threshold=color_cutoff,\n",
    "    above_threshold_color='k',\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=14,\n",
    "    labels = samplenames   \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cophenetic correlation\n",
    "The cophenetic correlation coefficient is a measure of how well the linkages established in hierarchical clustering represent the actual calculated distances between pairs of samples. We can use it to judge how well a dendrogram represents to the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining how well the dendrogram represents the cosine distances in the data\n",
    "ccorr, distances = cophenet(hc, pdist(df_transformed.T, metric))\n",
    "ccorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for presence in selected group vs absences in rest\n",
    "limit_category = 'ATTRIBUTE_collection_region'\n",
    "limit = 'Guam'\n",
    "\n",
    "category = 'ATTRIBUTE_collection_site'\n",
    "interest = 'Finger Reef'\n",
    "\n",
    "####################################################################\n",
    "file_limit = [c for c in df_transformed.columns if c in md[md[limit_category] == limit]['filename'].values]\n",
    "limited = df_transformed[file_limit]\n",
    "\n",
    "files_of_interest = [c for c in limited.columns if c in md[md[category] == interest]['filename'].values]\n",
    "\n",
    "df_transformed[np.max(limited.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "df_transformed[np.min(limited[files_of_interest].T) > 0].index\n",
    "\n",
    "query_result = [r for r in limited[np.max(limited.drop(files_of_interest, axis=1).T) <= 0].index if r in limited[np.min(limited[files_of_interest].T) > 0].index]\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit/filter df\n",
    "limit_category = 'ATTRIBUTE_collection_region'\n",
    "limit = 'Guam'\n",
    "dataframe = df_transformed\n",
    "\n",
    "####################################################################\n",
    "file_limit = [c for c in dataframe.columns if c in md[md[limit_category] == limit]['filename'].values]\n",
    "limited = dataframe[file_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for presence in selected group vs absences in rest [length of regionally-specific core feats]\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "interest = 'Saipan'\n",
    "\n",
    "####################################################################\n",
    "\n",
    "files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == interest]['filename'].values]\n",
    "\n",
    "empty_feats = df_transformed[np.max(df_transformed.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "core_metab = df_transformed[np.min(df_transformed[files_of_interest].T) > 0].index\n",
    "\n",
    "query_result = [r for r in core_metab if r in empty_feats]\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of core feats per region/other category\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "interest = 'Paracel Islands'\n",
    "\n",
    "####################################################################\n",
    "\n",
    "files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == interest]['filename'].values]\n",
    "\n",
    "# df_transformed[np.max(df_transformed.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "len(df_transformed[np.min(df_transformed[files_of_interest].T) > 0].index)\n",
    "\n",
    "# query_result = [r for r in df_transformed[np.max(df_transformed.drop(files_of_interest, axis=1).T) <= 0].index if r in df_transformed[np.min(df_transformed[files_of_interest].T) > 0].index]\n",
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed['BB3_BB3_01_57129.mzXML'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate table with core, specific core, avg + sd of features per sample, # of samples; PER CATEGORY\n",
    "\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "dataframe = df_transformed\n",
    "\n",
    "#############################################################################\n",
    "core_summary_df = pd.DataFrame()\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    files_of_interest = [c for c in dataframe.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    feats = [np.sum(dataframe[f] > 0) for f in files_of_interest]\n",
    "    empty_feats = dataframe[np.max(dataframe.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "    core_metab = dataframe[np.sum(dataframe[files_of_interest].T > 0)/len(files_of_interest) >= cutoff].index\n",
    "    query_result = [r for r in core_metab if r in empty_feats]\n",
    "\n",
    "    core_summary_df[cat] = [np.mean(feats), np.std(feats), len(core_metab), len(query_result), len(files_of_interest)]\n",
    "    \n",
    "core_summary_df.drop(['na','unknown','Panama', 'Puerto Rico'], axis=1, inplace=True)\n",
    "core_summary_df = core_summary_df.sort_index(axis=1)\n",
    "core_summary_df.index = ['mean - feats/sample', 'std - feats/sample', 'core feats', 'specific core feats', 'sample size']\n",
    "core_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_summary_df.to_csv('cores_summary_table_075.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembly dataframe of cores [all samples]\n",
    "category = 'ATTRIBUTE_collection_subregion'\n",
    "cutoff = 0.5\n",
    "dataframe = limited #usually df_transformed\n",
    "\n",
    "#################################################################\n",
    "all_core = []\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    files_of_interest = [c for c in dataframe.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    core_metab = dataframe[np.sum(dataframe[files_of_interest].T > 0)/len(files_of_interest) >= cutoff].index\n",
    "    all_core.extend(core_metab)\n",
    "    \n",
    "all_core = list(set(all_core))\n",
    "\n",
    "cores = dataframe.loc[all_core]\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembly dataframe of cores [by group]\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 0.75\n",
    "\n",
    "#################################################################\n",
    "all_core_df = pd.DataFrame()\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    all_core_df[cat] = np.sum(df_transformed[files_of_interest].T > 0)/len(files_of_interest) >= cutoff\n",
    "\n",
    "all_core_df.drop(['na','unknown','Panama', 'Puerto Rico'], axis=1, inplace=True)\n",
    "#all_core_df = all_core_df[np.max(all_core_df.T) >= cutoff]\n",
    "all_core_df = all_core_df[np.sum(all_core_df.sort_values(by='Papua New Guinea', ascending=False).T) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(all_core_df.sort_values(by='Paracel Islands', ascending=False), annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_cores = pd.DataFrame(index = all_core_df.columns, columns = all_core_df.columns)\n",
    "for col1 in all_core_df.columns:\n",
    "    for col2 in all_core_df.columns:\n",
    "        shared_core = len([m for m in all_core_df[all_core_df[col1]].index if m in all_core_df[all_core_df[col2]].index])\n",
    "        hm_cores.at[col1, col2] = shared_core\n",
    "\n",
    "hm_cores = hm_cores.sort_index(axis=0)        \n",
    "hm_cores = hm_cores.sort_index(axis=1)\n",
    "hm_cores_prop = hm_cores/np.max(hm_cores)\n",
    "hm_cores_prop = hm_cores_prop.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(hm_cores_prop, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_cores.to_csv('pariwise_cores_075.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_cores_prop.to_csv('pairwise_cores_prop_075.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rarefation and core-size tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rarefaction\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "\n",
    "#################################################################\n",
    "rarefaction = pd.DataFrame()\n",
    "labels = []\n",
    "samples = []\n",
    "feats = []\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    print(cat)\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    comps = []\n",
    "    for n in range(1, len(files_of_interest)+1):\n",
    "        comps.extend(list(itertools.combinations(files_of_interest, n)))\n",
    "\n",
    "    for comp in comps:\n",
    "        labels.append(cat) \n",
    "        samples.append(len(comp))\n",
    "        feats.append(np.sum(np.sum(df_transformed[list(comp)].T > 0) > 0))\n",
    "        \n",
    "rarefaction['labels'] = labels\n",
    "rarefaction['samples'] = samples\n",
    "rarefaction['features'] = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"samples\", y=\"features\",\n",
    "             hue=\"labels\",\n",
    "             data=rarefaction, estimator='mean', ci='sd')\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core-size vs sampling\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "\n",
    "#################################################################\n",
    "core_v_samp = pd.DataFrame()\n",
    "labels = []\n",
    "samples = []\n",
    "feats = []\n",
    "\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    print(cat)\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    comps = []\n",
    "    for n in range(1, len(files_of_interest)+1):\n",
    "        comps.extend(list(itertools.combinations(files_of_interest, n)))\n",
    "\n",
    "    for comp in comps:\n",
    "        labels.append(cat) \n",
    "        samples.append(len(comp))\n",
    "        feats.append(np.sum(np.sum(df_transformed[list(comp)].T > 0)/len(comp) >= cutoff))\n",
    "        \n",
    "core_v_samp['labels'] = labels\n",
    "core_v_samp['samples'] = samples\n",
    "core_v_samp['features'] = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"samples\", y=\"features\", hue=\"labels\",\n",
    "             data=core_v_samp, estimator='mean', ci='sd')\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "#ax.set_ylim(0,800)\n",
    "#ax.set_xlim(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core per mean samps\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "\n",
    "#################################################################\n",
    "core_v_samp = pd.DataFrame()\n",
    "labels = []\n",
    "samples = []\n",
    "feats = []\n",
    "core_vals = []\n",
    "props = []\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    print(cat)\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    comps = []\n",
    "    for n in range(1, len(files_of_interest)+1):\n",
    "        comps.extend(list(itertools.combinations(files_of_interest, n)))\n",
    "\n",
    "    for comp in comps:\n",
    "        labels.append(cat) \n",
    "        samples.append(len(comp))\n",
    "        feats.append(np.sum(np.sum(df_transformed[list(comp)].T > 0) > 0))\n",
    "        core_vals.append(np.sum(np.sum(df_transformed[list(comp)].T > 0)/len(comp) >= cutoff))\n",
    "        props.append(np.sum(np.sum(df_transformed[list(comp)].T > 0)/len(comp) >= cutoff)/np.sum(np.sum(df_transformed[list(comp)].T > 0) > 0))\n",
    "        \n",
    "core_v_samp['labels'] = labels\n",
    "core_v_samp['samples'] = samples\n",
    "core_v_samp['features'] = feats\n",
    "core_v_samp['core_va']\n",
    "core_v_samp['proportion'] = props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=\"samples\", y=\"proportion\", hue=\"labels\",\n",
    "             data=core_v_samp, estimator='mean', ci='sd')\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering - tabular output\n",
    "In some instances, it may be useful to have a tabular output of the hierarchical clustering that is visualized via dendrogram above. Generate such a table by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table output of hierarchical clustering\n",
    "hc_df = pd.DataFrame(hc)\n",
    "hc_df.columns = ['cluster 1', 'cluster 2', 'distance at merge', 'number of samples in new cluster']\n",
    "hc_df['new cluster number'] = [c for c in range(len(list(df_transformed.T.index)),len(list(df_transformed.T.index))+len(hc_df))]\n",
    "\n",
    "c1_alias = []\n",
    "for c in hc_df['cluster 1']:\n",
    "    if c < len(list(df_transformed.T.index)):\n",
    "        c1_alias.append(list(df_transformed.T.index)[int(c)])\n",
    "    else:\n",
    "        c1_alias.append('none')\n",
    "        \n",
    "c2_alias = []\n",
    "for c in hc_df['cluster 2']:\n",
    "    if c < len(list(df_transformed.T.index)):\n",
    "        c2_alias.append(list(df_transformed.T.index)[int(c)])\n",
    "    else:\n",
    "        c2_alias.append('none')\n",
    "\n",
    "\n",
    "hc_df['cluster 1 alias'] = c1_alias\n",
    "hc_df['cluster 2 alias'] = c2_alias\n",
    "hc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance matrix\n",
    "In some instances, it may also be useful to generate a pairwise distance matrix for the samples in the bucket table. Do so by running the cell below. Note: This will use the same distance metric as specified above in the hierarchical clustering parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise = pd.DataFrame(squareform(pdist(df_transformed.T, metric)))\n",
    "pairwise.index = df_transformed.T.index\n",
    "pairwise.columns = df_transformed.T.index\n",
    "pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection + Heat Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "For feature selection, ORCA uses univariate feature selection, as implemented in sklearn (https://scikit-learn.org/stable/). Univariate feature selection was selected as the optimal strategy of feature selection for this particular problem (e.g. identifying the most important features in driving clustering of different groups of samples) as it considers features individually and so is not negatively impacted by correlated features. In order to truly judge the statistical significance of features, univariate feature selection must be applied to a dataset with proper numbers of replicates per category of sample, and must also meet the three assumptions necessary for applying the ANOVA statistical test (here is a good resource for reviewing those assumptions: https://sites.ualberta.ca/~lkgray/uploads/7/3/6/2/7362679/slides_-_anova_assumptions.pdf) Even in cased where the assumptions are not met, or there are not enough replicate samples per category, feature selection can still be helpful for generating hypotheses about which features may be significant. To conduct univariate feature selection, please set the below parameters:\n",
    "\n",
    "**sample_labels**: Input the column name from the metadata table that you would like samples to be grouped by.\n",
    "\n",
    "**ref_for_labels**: This should be the column name whose contents exactly match the sample names in df_transformed. This will allow the sample (grouping) labels to be properly mapped to the samples.\n",
    "\n",
    "**top_k**: Number of top features to display.\n",
    "\n",
    "**rank_by**: Column to rank results by. This could be 'p-value' or 'F-values', if the goal is to see the most statistically significant features, as determined by univariate feature selection. \n",
    "\n",
    "**ascending**: Setting this variable to 'True' will list values from the designated column (as indicated with the 'rank_by' variable) from smallest to largest, while setting it to 'False' will list the values from largest to smallest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels = 'location'\n",
    "ref_for_labels = 'filename_path' #should be the column whose contents exactly match the sample names in df_transformed\n",
    "\n",
    "top_k = 5\n",
    "rank_by = 'Saipan'\n",
    "ascending = False\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "samplenames = [dict_labels[n] for n in df_transformed]\n",
    "\n",
    "df_featselect = df_transformed.copy(deep = True)\n",
    "df_featselect.index = list(df_transformed.index)\n",
    "\n",
    "F_values, p_values = sklearn.feature_selection.f_classif(df_featselect.T.values,samplenames)\n",
    "feat_select_results = pd.DataFrame(index = list(df_transformed.index))\n",
    "feat_select_results['F-values'] = F_values\n",
    "feat_select_results['p-values'] = p_values\n",
    "\n",
    "group_means = df_featselect.T.groupby(md['location'].values).agg('mean').T\n",
    "\n",
    "fs = pd.concat([feat_select_results, group_means], axis = 1).sort_values(by = rank_by, ascending=ascending).head(top_k)\n",
    "fs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map visualizations\n",
    "Heat maps can be generated for the feature selection output, or for an excerpt of the features vs samples bucket table.\n",
    "\n",
    "**feat_select_or_bucket_table**: Indicate either 'feat_select' or 'bucket_table', depending on which you would like a heat map generated of.\n",
    "\n",
    "If you select 'bucket_table', you can also indicate an m/z range for which features you would like included.\n",
    "\n",
    "**mz_high**: Upper limit of m/z for inclusion in the heat map\n",
    "\n",
    "**mz_low**: Lower limit of m/z for inclusion in the heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_select_or_bucket_table = 'bucket_table'\n",
    "mz_high = 460\n",
    "mz_low = 450\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "if feat_select_or_bucket_table == 'feat_select':\n",
    "    ax = sns.heatmap(fs.drop(['p-values', 'F-values'], axis=1))\n",
    "elif feat_select_or_bucket_table == 'bucket_table':\n",
    "    ax = sns.heatmap(df_transformed.query('mz > @mz_low and mz < @mz_high'))\n",
    "else:\n",
    "    print('invalid input. Try \"feat_select\" or \"bucket_table\".')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentative Identification of Features\n",
    "\n",
    "While m/z of an MS1 feature is not enough information to confidently identify what compound is in a sample, cross referencing features with lists of known compounds can allow for quick dereplication, which is very helfpul in the context of natural products discovery. The cell below allows one to load in a table of known compounds, their masses, and expected m/z for protenated and sodiated peaks, and then annotate features in your dataset based on this set of known compounds. By default, features are ordered based on which are the most prominent (e.g. which features have the largest values).\n",
    "\n",
    "The parameters to set are:\n",
    "\n",
    "**mz_tolerance**: How close a feature's m/z must be to the protenated or sodiated m/z of a known compound in order for the feature to be annotated as that known compound.\n",
    "\n",
    "**top_k**: Number of features to output. Note: features are ranked by greattest value to smallest value, as determined by selecting the max value for a feature across all samples.\n",
    "\n",
    "**db_path**: Path to table of known compounds to be used for annotation. Table must be a csv file with at least three columns: 'Name', 'Protenated Peak', and 'Sodiated Peak'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_tolerance = 1\n",
    "top_k = 10\n",
    "db_path = './Moorea_bouillonii_db.csv'\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "db = pd.read_csv(db_path)\n",
    "\n",
    "df_top = df_transformed.T.max().sort_values(ascending=False).head(top_k).reset_index()\n",
    "\n",
    "df_top.columns = ['mz', 'rt', 'max transformed integral']\n",
    "\n",
    "column_pids = []\n",
    "column_difs = []\n",
    "for m in df_top['mz']:\n",
    "    putative_ids = []\n",
    "    difs = []\n",
    "    for i in db.index:\n",
    "        if abs(m - db.iloc[i]['Protenated Peak']) <= mz_tolerance:\n",
    "            putative_ids.append(db.iloc[i]['Name'] + ' [M+H]+')\n",
    "            difs.append(round(abs(m - db.iloc[i]['Protenated Peak']),2))\n",
    "    for i in db.index:\n",
    "        if abs(m - db.iloc[i]['Sodiated Peak']) <= mz_tolerance:\n",
    "            putative_ids.append(db.iloc[i]['Name'] + ' [M+Na]+')\n",
    "            difs.append(round(abs(m - db.iloc[i]['Sodiated Peak']),2))\n",
    "    if len(putative_ids) == 0:\n",
    "        putative_ids.append('None')\n",
    "        difs.append(0)\n",
    "    column_pids.append(putative_ids)\n",
    "    column_difs.append(difs)\n",
    "    \n",
    "df_top['putative ids'] = column_pids\n",
    "df_top['difference'] = column_difs\n",
    "df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous \n",
    "Here are some bits of code that might be useful. Feel free to add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of features per sample, post transformations\n",
    "df_transformed[df_transformed > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export a dataframe to csv\n",
    "df_transformed.to_csv('bucket_table-transformed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
