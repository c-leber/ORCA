{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME TO THE ORCA NOTEBOOK\n",
    "## *Moorena bouillonii* Chemogeography Expansion\n",
    "\n",
    "This branch of ORCA was used for the 4th Chapter of my PhD Dissertation- *Moorena bouillonii* Chemogeography: Distributional Patterns of Compounds and Compound Families at Multiple Spatial Scales.\n",
    "Please excuse the digital dust - the whirlwind of finishing my PhD has not yet allowed for me to get everything cleaned up yet. Feel free to check out the code, but if you would like a more user-friendly experience, please see the master branch!\n",
    "_______________________________________________________________________________________________________________________________\n",
    "\n",
    "Pipeline for the **O**bjective **R**elational **C**omparative **A**nalyses of mass spectral data, along with other data sources. All you need is a directory of mzXML files or a features vs samples bucket table to get started!\n",
    "\n",
    "To run cells of code, select cell and then press **Shift + Enter**. The first cell loads-in python modules necessary for the rest of the code to function\n",
    "\n",
    "In some of the cells below, the user will need to input information, such as setting paths, setting parameters, etc. In these cases, the user will see a cell with variables to be set at the top, followed by a line of '###', below which the rest of the code can be seen. Please set all applicable variables above the '###' line. Tinkering with the code below the '###' line is highly encouraged, as that is precisely why we chose to make it available as a Jupyter Notebook, however it could result in a 'breaking' of ORCA. If that appears to be the case, simply clone again from GitHub to get back to working code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyteomics import mzxml, auxiliary\n",
    "import glob\n",
    "from scipy.spatial.distance import pdist, squareform, cosine\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from statistics import mean\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, set_link_color_palette\n",
    "from collections import Counter\n",
    "import sklearn.feature_selection\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=4, edgeitems=5, linewidth=100, suppress=True, threshold=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS1 Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains the first variable that you need to assign, if you are using ORCA to do MS1 feature processing - the path to the folder (directory) that contains your samples. If this notebook is in the same folder as your samples, the path should be './'. After you set the path, run the cell (**Shift + Enter**) and it should display a list of your mzXML sample files. If your files are not displayed, your path may be wrong.\n",
    "\n",
    "Note: If you already have a features vs samples bucket table, and are not trying to use ORCA for MS1 Feature Processing, please proceed to the section in the notebook titled **Analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sample_directory = '../Guam_Mb_comps/' #last character should always be '/'\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "#Load files from the indicated directory with .mzXML extension\n",
    "fns = glob.glob(path_to_sample_directory +'*.mzXML')\n",
    "fns.sort()\n",
    "peak_list = []\n",
    "peak_range_low = []\n",
    "peak_range_high = []\n",
    "rt_list = []\n",
    "rt_range_low = []\n",
    "rt_range_high = []\n",
    "rrt_list = []\n",
    "rrt_range_low = []\n",
    "rrt_range_high = []\n",
    "integ = []\n",
    "sample = []\n",
    "fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata for mzXML files\n",
    "This cell contains another variable for you to assign - the path to a metadata file, that has information about your samples. At minimum, this file needs a column titled 'filename' that includes the filename (including the .mzXML extension) for each of your samples. If this notebook is in the same folder as your metadata file, the path should be './*name-of-metadata-file.csv*'. After you set the path, run the cell and it should display your metadata file as a table, with two additional columns added that include modified versions of the filenames - these are important for mapping metadata later on in the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_metadata = '../Guam_Mb_comps/metadata.csv'\n",
    "\n",
    "##############################################################################################################\n",
    "#load in metadata file, mapping file names to other information\n",
    "\n",
    "md = pd.read_csv(path_to_metadata)\n",
    "md['filename_stripped'] = [f.split('.mzXML')[0] for f in md['filename']]\n",
    "md['filename_path'] = [path_to_sample_directory + f for f in md['filename']]\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom retention time function\n",
    "The original intent behind the design of ORCA was to be able to perform MS1 analyses on heterogeneous datasets (e.g. comparing data files acquired at different time points, under different chromatographic conditions, etc). In particular, we were faced with comparing current samples and legacy samples that had been run on similar but not the same chromatgraphic method (namely, a 2-min increase in run length to compensate for a XX decrease in flow-rate). This meant that using retention time as a second dimension (along with m/z) to charcaterize MS1 features did not allow of the proper consolidation of MS1 features across different samples. We were able to overcome this by instead using relative retention, along with applying an appropriate relative retention time tolerance window as determined through manual validation.\n",
    "\n",
    "Datasets that are more chromatographically hterogeneous than ours will likely require more complex functions applied to retention time in order for it to remain a reasonable and reliable second dimension by which MS1 features can be consolidated. We provide the opportunity to apply custom retention time functions in the cell below. Some ideas are: XX\n",
    "\n",
    "As is always the case when setting parameters or applying custom functions in tools, it is the user's responsibility to manually validate that appropriate parameters and functions have been applied. What this specifically means is checking that features known to be the same across different samples are in fact being considered the same in the resultant features vs samples bucket table. If this is not the case, please change the parameters/functions. The best guidance on selecting appropriate parameters and functions comes from studying and identifying the nuances in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom retention time function\n",
    "\n",
    "def custom_rt(rt, total_run_time):\n",
    "    return rt/total_run_time\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paramters for MS1 feature picking\n",
    "\n",
    "Here are descriptions and recommendations for the different parameters to be set\n",
    "\n",
    "**bin_width**: The first step in generating feature lists for each sample is to divide the range of m/z values to be considered into bins, sorting features into these bins based on m/z, and then grouping peaks within each bin based on those peaks occurring in consecutive MS1 scans. Bin width determines the size of these bins. Appropriately setting bin width depends on the resolution of the instrument(s) from which data was acquired, and is essentially a form or m/z tolerance. For low resolution instruments/heterogenous sample sets, 1 is a safe starting point.\n",
    "\n",
    "**bin_offset**: This parameter is unnecessary for most uses, and so should defaultly be set to 0. It is useful in cases where a systematic bias in m/z is seen in the data, e.g. if a feature that should have an m/z of 100.0 is consistently being recorded as 100.3, a bin_offset of 0.3 may be appropriate.\n",
    "\n",
    "**bins_start**: The lowest m/z to be considered.\n",
    "\n",
    "**bins_end**: The highest m/z to be considered.\n",
    "\n",
    "**peak_consecutivity**: This parameter designates how 'consecutive' peaks need to be in order to be considered as composing the same feature. A value of 0 means that peaks will only be considered as part of a singular feature if there are 0 MS1 scans between them in which the peak does not appear. A value of 1 means that if a peak does not appear in one MS1 scan, but it does appear in the scans directly before and after, those instances can will still be grouped together as one feature. For guidedance in setting this parameter, take a look at your data - do peaks tend to occur in consecutive MS1 scans, or are there sometimes gaps?\n",
    "\n",
    "**peak_cluster_size_cutoff**: This parameter designates how wide (e.g. how many consecutive scans it is found across) a potential feature needs to be in order to be considered a true feature. For example, setting this parameter to 3 means that a potential feature must be recorded in three or more scans meeting the consecutivity criyteria designated via the peak_consecutivity parameter in order to be considered a true feature.\n",
    "\n",
    "**min_integral**: Minimum integral a potential feature must have in order to be considered a true feature.\n",
    "\n",
    "**rt_setting**: This parameter determines what is used as the second dimension, along with m/z, is used for defining specific features. The options are 'raw' for using regular retention time, 'relative' for using relative retention time, or 'custom' for applying a custom retention time function that the user can define above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter MS1 feature picking paramaters:\n",
    "bin_width = 1\n",
    "bin_offset = 0 \n",
    "bins_start = 200\n",
    "bins_end = 2000\n",
    "peak_consecutivity = 0\n",
    "peak_cluster_size_cutoff = 3\n",
    "min_integral = 100000\n",
    "\n",
    "rt_setting = 'raw'\n",
    "\n",
    "##########################################################################################################################\n",
    "#From each file, bin and integrate peaks from all MS1 scans\n",
    "if rt_setting not in ['relative', 'raw', 'custom']:\n",
    "    print('rt_setting is not properly set. The options are \"raw\", \"relative\", or \"custom\". Please try again.')\n",
    "    raise SystemExit(0)\n",
    "for file_name in fns:\n",
    "    print(file_name)\n",
    "    rt = []\n",
    "    run_length = []\n",
    "    mz = []\n",
    "    inten = []\n",
    "    with mzxml.read(file_name) as reader:\n",
    "        for spectrum in reader:\n",
    "            run_length.append([spectrum['retentionTime']])\n",
    "            if spectrum['msLevel'] == 1:\n",
    "                rt.extend([spectrum['retentionTime']] * len(spectrum['m/z array']))\n",
    "                mz.extend(spectrum['m/z array'])\n",
    "                inten.extend(spectrum['intensity array'])\n",
    "    run_length = np.max(run_length)            \n",
    "    rt = np.array(rt, dtype = np.float32)\n",
    "    rt_unique = np.unique(rt)\n",
    "    rt_rank = pd.factorize(rt)[0]\n",
    "    mz = np.array(mz, dtype = np.float32)\n",
    "    inten = np.array(inten, dtype = np.float32)\n",
    "    \n",
    "    bins = np.arange(bins_start, bins_end, bin_width)\n",
    "    for b in bins:\n",
    "        rt_positions = rt_rank[np.where((mz > (b - (bin_width/2) + bin_offset)) * (mz < (b + (bin_width/2) + bin_offset)))[0]]\n",
    "        rt_positions = np.array(list(set(rt_positions)))\n",
    "        #cluster rt_positions when they are consecutive\n",
    "        clusters = list(np.split(rt_positions, np.where(np.diff(rt_positions) > peak_consecutivity+1)[0]+1))\n",
    "        cluster_size = np.array([len(c) for c in clusters])\n",
    "        clusters_filt = [clusters[i] for i in list(np.where(cluster_size > peak_cluster_size_cutoff)[0])]\n",
    "        for y in clusters_filt:\n",
    "            idx = np.where((mz > (b - (bin_width/2) + bin_offset )) * (mz < (b + (bin_width/2) + bin_offset )) * (rt_rank >= np.min(y)) * (rt_rank <= np.max(y)))[0]\n",
    "            xcoord = rt[idx]\n",
    "            ycoord = inten[idx]\n",
    "            integral = np.trapz(y = ycoord, x = xcoord)\n",
    "            if integral > min_integral:\n",
    "                integ.append(integral)\n",
    "                peak_list.append(np.mean(mz[idx]))\n",
    "                peak_range_low.append(np.min(mz[idx]))\n",
    "                peak_range_high.append(np.max(mz[idx]))\n",
    "                rt_list.append(np.mean(rt[idx]))\n",
    "                rt_range_low.append(np.min(rt[idx]))\n",
    "                rt_range_high.append(np.max(rt[idx]))\n",
    "                sample.append(file_name)\n",
    "                if rt_setting == 'relative':\n",
    "                    rrt_list.append(np.mean(rt[idx])/run_length)\n",
    "                    rrt_range_low.append(np.min(rt[idx]/run_length))\n",
    "                    rrt_range_high.append(np.max(rt[idx]/run_length))\n",
    "                elif rt_setting == 'raw':\n",
    "                    rrt_list.append(np.mean(rt[idx]))\n",
    "                    rrt_range_low.append(np.min(rt[idx]))\n",
    "                    rrt_range_high.append(np.max(rt[idx]))\n",
    "                elif rt_setting == 'custom':\n",
    "                    rrt_list.append(custom_rt(np.mean(rt[idx]), run_length))\n",
    "                    rrt_range_low.append(custom_rt(np.min(rt[idx]), run_length))\n",
    "                    rrt_range_high.append(custom_rt(np.max(rt[idx]), run_length))\n",
    "                else:\n",
    "                    print('rt_setting is not properly set. The options are \"raw\", \"relative\", or \"custom\". Please try again.')\n",
    "                    raise SystemExit(0)\n",
    "#         print('current bin: ' + str(b))\n",
    "#         print('current length of sample: ' + str(len(sample)))\n",
    "#         print('current length of rrt_list: ' + str(len(rrt_list)))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paramters for MS1 feature consolidation\n",
    "\n",
    "Here are descriptions and recommendations for the different parameters to be set\n",
    "\n",
    "**rrt_tolerance**: This is the maximum raw/relative/custom retention time distance between the edges of two features in order for them to be candidates for consolidation.\n",
    "\n",
    "**bin_width**: Bin width, again, determines the size of m/z feature bins. Appropriately setting bin width depends on the resolution of the instrument(s) from which data was acquired, and is essentially a form or m/z tolerance. For low resolution instruments/heterogenous sample sets, 1 is a safe starting point.\n",
    "\n",
    "**bin_offset**: This parameter is unnecessary for most uses, and so should defaultly be set to 0. It is useful in cases where a systematic bias in m/z is seen in the data, e.g. if a feature that should have an m/z of 100.0 is consistently being recorded as 100.3, a bin_offset of 0.3 may be appropriate.\n",
    "\n",
    "**bins_start**: The lowest m/z to be considered.\n",
    "\n",
    "**bins_end**: The highest m/z to be considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add feature consolidation parameters\n",
    "rrt_tolerance = 0.5 #maximum rrt distance between edges of two feature peaks in order for them to be candidates for consolidation\n",
    "bin_width = 1\n",
    "bin_offset = 0 #must be less than half of bin_width\n",
    "bins_start = 200\n",
    "bins_end = 2000\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#Peak consoslidating\n",
    "\n",
    "masses = np.arange(bins_start, bins_end, bin_width)\n",
    "peak_list = np.array(peak_list, dtype = np.float32)\n",
    "rrt_list = np.array(rrt_list, dtype = np.float32)\n",
    "rrt_range_high = np.array(rrt_range_high, dtype = np.float32)\n",
    "rrt_range_low = np.array(rrt_range_low, dtype = np.float32)\n",
    "\n",
    "updated_integ = []\n",
    "updated_peak_list = []\n",
    "updated_rrt_list = []\n",
    "updated_rrt_range_low = []\n",
    "updated_rrt_range_high = []\n",
    "updated_sample = []\n",
    "temp_peaks = []\n",
    "\n",
    "#search through each mass, and collapse peaks across samples, based on rrt tolerance/overlap\n",
    "for mass in masses:\n",
    "    print(mass)\n",
    "    idx = list(np.where((peak_list > (mass - (bin_width/2) + bin_offset)) * (peak_list < (mass + (bin_width/2) + bin_offset)))[0])\n",
    "    i_record = []\n",
    "    for i in idx:\n",
    "        temp_peaks_flag = 0\n",
    "        for j in idx:\n",
    "            if j != i:\n",
    "                if sample[i] != sample[j] and max(rrt_range_low[i],rrt_range_low[j]) < min(rrt_range_high[i],rrt_range_high[j]) + rrt_tolerance:\n",
    "                    temp_peaks_flag += 1\n",
    "                    temp_peaks.append([i,j])\n",
    "\n",
    "        i_record.append(i)\n",
    "        if temp_peaks_flag == 0:\n",
    "            updated_integ.append(integ[i])\n",
    "            updated_peak_list.append(peak_list[i])\n",
    "            updated_rrt_list.append(rrt_list[i])\n",
    "            updated_rrt_range_low.append(rrt_range_low[i])\n",
    "            updated_rrt_range_high.append(rrt_range_high[i])\n",
    "            updated_sample.append(sample[i])\n",
    "            \n",
    "\n",
    "for tp in temp_peaks:\n",
    "    tp.sort()\n",
    "    \n",
    "temp_peaks.sort()\n",
    "temp_peaks = list(tp for tp,_ in itertools.groupby(temp_peaks))\n",
    "\n",
    "#collapse peaks by graphing overlaps\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(temp_peaks)\n",
    "apeaks = list(nx.connected_components(G))\n",
    "\n",
    "#update peak lists to include adjusted mz and rrt for collapsed peaks\n",
    "for apeak in apeaks:\n",
    "    apeak = list(apeak)\n",
    "    curr_integ = [integ[a] for a in apeak]\n",
    "    curr_peak_list = [peak_list[a] for a in apeak]\n",
    "    curr_rrt_list = [rrt_list[a] for a in apeak]\n",
    "    curr_rrt_range_low = [rrt_range_low[a] for a in apeak]\n",
    "    curr_rrt_range_high  = [rrt_range_high[a] for a in apeak]\n",
    "    curr_sample = [sample[a] for a in apeak]\n",
    "    for file_name in list(set(curr_sample)):\n",
    "        csidx = [i for i, j in enumerate(curr_sample) if j == file_name]\n",
    "        updated_integ.append(sum([curr_integ[c] for c in csidx]))\n",
    "        updated_peak_list.append(np.mean(curr_peak_list))\n",
    "        updated_rrt_list.append(np.mean(curr_rrt_list))\n",
    "        updated_rrt_range_low.append(np.mean(curr_rrt_range_low))\n",
    "        updated_rrt_range_high.append(np.mean(curr_rrt_range_high))\n",
    "        updated_sample.append(file_name)\n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bucket table\n",
    "This next cell organizes features into a features vs samples bucket table, to which analyses can then be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate empty Pandas dataframe, to be populated as a bucket table\n",
    "columns = fns\n",
    "index0 = list(set(list(zip(updated_peak_list, updated_rrt_list))))\n",
    "index0.sort()\n",
    "df_updated = pd.DataFrame(index=index0, columns=columns)\n",
    "df_updated = df_updated.fillna(0)\n",
    "\n",
    "#populate bucket table\n",
    "updated_sample = np.array(updated_sample)\n",
    "updated_integ = np.array(updated_integ)\n",
    "for mz_rrt in index0:\n",
    "    for filename in fns:\n",
    "        idx_to_df = np.where((updated_peak_list == mz_rrt[0]) * (updated_rrt_list == mz_rrt[1]) * (updated_sample == filename))[0]\n",
    "        if len(idx_to_df) == 1:\n",
    "            value = updated_integ[idx_to_df]\n",
    "            df_updated.at[mz_rrt, filename] = value\n",
    "\n",
    "df_updated.index = pd.MultiIndex.from_tuples(index0, names=['mz', 'rrt'])\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_updated > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~ End of MS1 Feature Processing ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-in features vs samples bucket table\n",
    "\n",
    "If you already have a features vs samples bucket table, you can load it into ORCA to perform analyses here. Input the path to your metadata table in the cell below. It should be either in tsv or csv format. If you load-in your bucket table and see that it includes columns or rows that it should not, add those columns or rows to a list under the variable **drop_columns** or **drop_rows**, respectively. By setting **col_ref** and/or **row_ref** as 'True', an auxilary dataframe will be produced that contains those dropped columns/rows for future reference. If there is prefix or suffix included in the sample names included in the table, you can remove those by adding that string to be removed in **table_name_trim**. \n",
    "\n",
    "Note: All of the following code is setup to work on a bucket table that is arranged so that the features are represented by each row, and the samples are represented by each column. If your bucket table is arranged in the alternative fashion (rows = samples, columns = features), please set **transpose_buckettable** to True.\n",
    " \n",
    "**If you used the MS1 Feature Processing to generate a bucket table above, you can skip the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckettable_path = '../../MZmine-2.53-Windows/Mb_MS_0_quant.csv'\n",
    "drop_columns = ['Unnamed: 159', 'row ID', 'row m/z', 'row retention time']\n",
    "col_ref = True\n",
    "drop_rows = []\n",
    "row_ref = False\n",
    "sample_name_trim = ' Peak area'\n",
    "\n",
    "transpose_buckettable = False\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if buckettable_path.split('.')[-1] == 'tsv':\n",
    "    bt = pd.read_table(buckettable_path)\n",
    "elif buckettable_path.split('.')[-1] == 'csv':\n",
    "    bt = pd.read_csv(buckettable_path)\n",
    "else:\n",
    "    print('Format invalid. File must be csv or tsv.')\n",
    "    \n",
    "if len(drop_columns) > 0:\n",
    "    if col_ref == True:\n",
    "        col_ref_df = bt[drop_columns]\n",
    "    bt.drop(drop_columns, axis = 1, inplace=True)\n",
    "\n",
    "if len(drop_rows) > 0:\n",
    "    if row_ref == True:\n",
    "        row_ref_df = bt.iloc[drop_rows]\n",
    "    bt.drop(drop_rows, axis = 0, inplace=True)\n",
    "\n",
    "if transpose_buckettable == True:\n",
    "    bt = bt.T\n",
    "    \n",
    "bt.columns = bt.columns.str.replace(sample_name_trim, '')\n",
    "\n",
    "df_updated = bt.copy(deep=True)\n",
    "df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Load-in buckettable, normalize to internal reference, and re-export\n",
    "If one finds themself needing to normalize a buckettable to the internal standard peak, and then re-exporting for use in another tool (e.g. GNPS feature-based molecular network), you can do that here!\n",
    "\n",
    "**IS_ref_column**: name of column to look at for referencing the internal standard peak/other peak for normalizing against\n",
    "\n",
    "**IS_ref_value**: the value used to indicate which row is the internal standard/other peak for normalizing against\n",
    "\n",
    "**export**: Set to 'True' for exporting the normalized table as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckettable_path = '../../MZmine-2.53-Windows/Mb_MS_0_gnpsfilt_quant.csv'\n",
    "drop_columns = ['Unnamed: 159', 'row ID', 'row m/z', 'row retention time']\n",
    "col_ref = True\n",
    "drop_rows = []\n",
    "row_ref = False\n",
    "sample_name_trim = ''\n",
    "\n",
    "transpose_buckettable = False\n",
    "\n",
    "IS_ref_column = 'row ID'\n",
    "IS_ref_value = 1454\n",
    "\n",
    "export = True\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "if buckettable_path.split('.')[-1] == 'tsv':\n",
    "    bt = pd.read_table(buckettable_path)\n",
    "elif buckettable_path.split('.')[-1] == 'csv':\n",
    "    bt = pd.read_csv(buckettable_path)\n",
    "else:\n",
    "    print('Format invalid. File must be csv or tsv.')\n",
    "    \n",
    "IS_index = bt[bt[IS_ref_column] == IS_ref_value].index\n",
    "\n",
    "if len(drop_columns) > 0:\n",
    "    if col_ref == True:\n",
    "        col_ref_df = bt[drop_columns]\n",
    "    bt.drop(drop_columns, axis = 1, inplace=True)\n",
    "\n",
    "if len(drop_rows) > 0:\n",
    "    if row_ref == True:\n",
    "        row_ref_df = bt.iloc[drop_rows]\n",
    "    bt.drop(drop_rows, axis = 0, inplace=True)\n",
    "    \n",
    "if transpose_buckettable == True:\n",
    "    bt = bt.T\n",
    "    \n",
    "bt.columns = bt.columns.str.replace(sample_name_trim, '')\n",
    "\n",
    "replace_val = bt.loc[IS_index].T.mean().values[0]\n",
    "bt.loc[IS_index] = bt.loc[IS_index].replace(0,replace_val)\n",
    "bt = bt/np.array(bt.loc[IS_index])\n",
    "\n",
    "## NEED TO FIGURE OUT HOW TO BEST ORGANIZE THIS\n",
    "df_IS_normed = pd.concat([col_ref_df, bt], axis=1).drop(['Unnamed: 159'], axis = 1)\n",
    "df_IS_normed.to_csv('../../MZmine-2.53-Windows/Mb_MS_0_gnpsfilt_quant_ISnormed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata for features vs samples bucket table\n",
    "This cell contains another variable for you to assign - the path to a metadata file, that has information about the samples in your bucket table. At minimum, this file needs a column with the names of your samples, matching how they are named in the bucket table. This column will be used to map any other metadata to the samples. \n",
    "\n",
    "**If you used the MS1 Feature Processing to generate a bucket table above, and you have already loaded-in a metadata table above, you can skip the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_metadata = '../../ReDU_v3.0_Mb_extracts_draft.tsv'\n",
    "\n",
    "#############################################################################\n",
    "#load in metadata file, mapping file names to other information\n",
    "if path_to_metadata.split('.')[-1] == 'tsv':\n",
    "    md = pd.read_table(path_to_metadata)\n",
    "elif path_to_metadata.split('.')[-1] == 'csv':\n",
    "    md = pd.read_csv(path_to_metadata)\n",
    "\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom transformation for your data\n",
    "Below, a number of transformations can be applied to your bucket table. If you cannot find a transformation that meets your needs, feel free to create a custom transformation in the cell directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformation\n",
    "def custom_transform(df_transformed):\n",
    "    #Edit here#\n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "Options for transformations to be applied to your bucket table are log (base 10) and natural log transformations, unit vector normalization, conversion of feature values to proportions per sample, conversion of feature values to presence-absence data, or any custom transformation defined by the user. In order to apply a transformation, set the applicable variable to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "log_10_transform = True\n",
    "log_natural_transform = False\n",
    "\n",
    "unit_vector_normalization = False\n",
    "transform_to_proportions = False\n",
    "presence_absence = False\n",
    "\n",
    "custom_transform = False\n",
    "\n",
    "#############################################################################\n",
    "df_transformed = df_updated.copy(deep = True)\n",
    "flag_transf = 0\n",
    "\n",
    "if log_10_transform == True and log_natural_transform == True:\n",
    "    flag_transf = 1\n",
    "    print('Cannot use both log transforms at the same time.')\n",
    "\n",
    "#log (base 10) transform integrals\n",
    "if log_10_transform == True and flag_transf == 0:\n",
    "    df_transformed[df_transformed < 100] = 1\n",
    "    df_transformed = np.log10(df_transformed)\n",
    "    \n",
    "#log (natural) transform integrals\n",
    "if log_natural_transform == True and flag_transf == 0:\n",
    "    df_transformed[df_transformed < 100] = 1\n",
    "    df_transformed = np.log10(df_transformed)\n",
    "    \n",
    "# Unit vector normalization\n",
    "if unit_vector_normalization == True:\n",
    "    df_transformed = df_transformed/(df_transformed**2).sum()**0.5\n",
    "    \n",
    "# Change integrals to proportions\n",
    "if transform_to_proportions == True:\n",
    "    df_transformed = df_transformed/df_transformed.sum()\n",
    "    \n",
    "# Change presence-absence (1-0) data\n",
    "if presence_absence == True:\n",
    "    df_transformed[df_transformed > 0] = 1\n",
    "    \n",
    "    \n",
    "#Applies custom transform\n",
    "if custom_transform == True:\n",
    "    df_transformed = custom(df_transformed)\n",
    "    \n",
    "df_transformed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Set Filtering/Additional Preprocessing Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features based on certain label (e.g. blanks)\n",
    "I used this code snippet to remove features associated with different types of blanks in my dataset. \n",
    "\n",
    "**filterby**: name of column that contains labels of groups whose features you would like to remove\n",
    "\n",
    "**label_to_drop**: list of labels for groups whose features should be dropped\n",
    "\n",
    "**filename_column**: column that contains filenames. These should be filenames in the format that is shard between the buckettable and the metadata dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features from blanks [indicate process, solvent, qc_mix, and internal_standard seperately  **Order does not matter]\n",
    "filterby = 'BlankType'\n",
    "label_to_drop = ['process','solvent','qc_mix','internal_standard']\n",
    "filename_column = 'filename'\n",
    "\n",
    "################################################################################################\n",
    "for blank in label_to_drop:\n",
    "    query = filterby + ' == \"' + blank + '\"'\n",
    "    print(blank)\n",
    "    print(len(df_transformed[md.query(query)['filename']][df_transformed[md.query(query)['filename']].T.median() > 0]))\n",
    "    df_transformed.drop(df_transformed[md.query(query)['filename']][df_transformed[md.query(query)['filename']].T.median() > 0].index, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted samples - method 1\n",
    "I used this code to remove blanks (after I had used them for removing features that could be found in blanks), so that I could carry on analysis only of samples I was interested in.\n",
    "\n",
    "**filterby**: name of metadata column that contains labels of samples you would like to remove\n",
    "\n",
    "**label_to_drop**: label of smaples you would like to drop. The code is setup to keep whatever samples do not have this label.\n",
    "\n",
    "**filename_column**: column of filenames - this column serves as the identifier that allows linking data from the metadata tabel to the buckettable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blanks \n",
    "filterby = 'SampleType'\n",
    "label_to_drop = 'blank_QC'\n",
    "filename_column = 'filename'\n",
    "\n",
    "################################################################################################\n",
    "query = filterby + ' != \"' + label_to_drop + '\"'\n",
    "df_transformed = df_transformed[md.query(query)['filename']]\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted samples - method 2\n",
    "I used this code to remove samples from locations that were outside the range of *M. bouillonii*, so that I could carry on analysis only of samples I was interested in.\n",
    "\n",
    "**filterby**: name of metadata column that contains labels of samples you would like to remove\n",
    "\n",
    "**label_to_drop**: label of smaples you would like to drop. The code is setup to drop samples that do have this label.\n",
    "\n",
    "**filename_column**: column of filenames - this column serves as the identifier that allows linking data from the metadata tabel to the buckettable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non Mb (Panama, unknown, Puerto Rico)\n",
    "filterby = 'ATTRIBUTE_collection_region'\n",
    "label_to_drop = 'unknown'\n",
    "filename_column = 'filename'\n",
    "\n",
    "################################################################################################\n",
    "query = filterby + ' == \"' + label_to_drop + '\"'\n",
    "df_transformed.drop(list(md.query(query)['filename']), axis = 1, inplace=True)\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features not included in both sample duplicates\n",
    "Even after intensive preprocessing, MS data can still be quite noisey. Some of this noise can be reduced by running samples in duplicate. This code block removes peaks that appear in one duplicate, but not in the other.\n",
    "\n",
    "***NOTE***: This code is currently set to only work in the situation of duplicates (not triplicate +), and only if the following naming convention is used to differentiate between duplicates:\n",
    "\n",
    "**Duplicate 1**: SampleName_01_AnythingElseIncluding.Extension\n",
    "\n",
    "**Duplicate 2**: SampleName_02_AnythingElseIncluding.Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove peaks not present in both reps (remove blanks first!!)\n",
    "cols = list(df_transformed.columns)\n",
    "samps = list(set([c.split('_0')[0] for c in cols]))\n",
    "for s in samps:\n",
    "    print(s)\n",
    "    for i in df_transformed.index:\n",
    "        if sum(df_transformed[df_transformed.filter(like=s).columns].loc[i] > 0) != 2:\n",
    "            df_transformed.at[i, df_transformed.filter(like=s).columns[0]] = 0\n",
    "            df_transformed.at[i, df_transformed.filter(like=s).columns[1]] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buckettable Checkpoint\n",
    "Removing peaks that are not present in both duplicates, along with other preprocessing and filtering steps, can take a long time. In order to set aside a dataframe copy, in case one wants to return to it after making additional changes, use the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_transformed.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve dataframe from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_0.copy(deep=True)\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sample duplicates from dataset\n",
    "I used this code after removing peaks not present in both duplicates, making duplicates consistent and essentially identical. Therefore, one of each duplicate could be dropped, as it was not contributing any new information to analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dups\n",
    "df_transformed = df_transformed[df_transformed.filter(like='_01').columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop empty features\n",
    "After various going through various filtering steps, the situation can occur where one has features in the table that did not have any observed peaks in the samples currently included in the dataset. This code cleans things up by removing those empty features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop COMPLETELY empty features\n",
    "df_transformed.drop(df_transformed[df_transformed.T.sum() < 0.00000000001].index, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop peaks below a certain size\n",
    "\n",
    "**min_peak**: peaks with area below or equal to this number will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak cutoff\n",
    "min_peak = 6.5\n",
    "\n",
    "##########################################################################\n",
    "df_transformed = df_transformed.apply(lambda x: np.where(x < 6.5,0,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features based on frequency in dataset\n",
    "\n",
    "**freq**: list of integers, indicating frequencies to be dropped, e.g. if you have 50 samples and you wish to drop features that are present in all 50 samples, and 50 to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features based on frequency across samples\n",
    "freq = [49,48]\n",
    "######################################################\n",
    "for f in freq:\n",
    "    df_transformed.drop(df_transformed[np.sum(df_transformed.T > 0) == f].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features not in the top k peaks of samples\n",
    "If a feature is counted amongst the top k peaks of any sample, it (and all of the peaks associated with that feature, regardless of rank, will be retained)\n",
    "\n",
    "**k**: number of top peaks to keep for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all features not in the top k peaks for a sample\n",
    "k = 250\n",
    "########################################################\n",
    "\n",
    "index_to_keep = []\n",
    "\n",
    "for c in df_transformed.columns:\n",
    "    cutoff = df_transformed[c].sort_values(ascending=False).iloc[k]\n",
    "    index_to_keep.extend(list(df_transformed[df_transformed[c] > cutoff].index))\n",
    "\n",
    "index_to_keep = list(set(index_to_keep))\n",
    "df_transformed = df_transformed.loc[index_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop peaks no in the top k peaks, per sample\n",
    "This will set peaks to 0 that are not in the top k peaks of each sample\n",
    "\n",
    "**k**: number of top peaks to keep for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all peaks not in top k\n",
    "k = 20\n",
    "\n",
    "for c in df_transformed.columns:\n",
    "    print(np.sum(df_transformed[c] > 0))\n",
    "    cutoff = df_transformed[c].sort_values(ascending=False).iloc[k]\n",
    "    df_transformed[c] = df_transformed[c].apply(lambda x: np.where(x <= cutoff,0,x))\n",
    "    print(np.sum(df_transformed[c] > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features no in the top k features of the entire dataset\n",
    "This sums up features across the dataset, and then ranks them, allowing you to drop features not in the top k\n",
    "\n",
    "**k**: number of features to retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features not in top k\n",
    "k = 150\n",
    "df_transformed = df_transformed[np.sum(df_transformed.T) > np.sum(df_transformed.T).sort_values(ascending=False).iloc[k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit or filter dataset to only include some groups\n",
    "\n",
    "**limit_category**: metadata column that includes labels for group that you would like to limit the dataset to\n",
    "\n",
    "**limit**: group label that you would like to limit the dataset to\n",
    "\n",
    "**dataframe**: dataframe that you would like to limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit/filter df\n",
    "limit_category = 'ATTRIBUTE_collection_region'\n",
    "limit = 'Guam'\n",
    "dataframe = df_transformed\n",
    "\n",
    "####################################################################\n",
    "file_limit = [c for c in dataframe.columns if c in md[md[limit_category] == limit]['filename'].values]\n",
    "limited = dataframe[file_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)\n",
    "\n",
    "I have found that applying unit vector transform prior to PCA is helpful. Run the cell below to do so.\n",
    "\n",
    "**dataframe**: name of dataframe to use\n",
    "\n",
    "**sample_labels**: metadata column including labels to colorize samples by in PCA plot\n",
    "\n",
    "**ref_for_labels**: metadata column with sample names; used for mapping metadata to buckettable\n",
    "\n",
    "**x_label**: axis title\n",
    "\n",
    "**y_label**: axis title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit vector transform prior to PCA\n",
    "df_transformed = df_transformed/(df_transformed**2).sum()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build PCA (10 components), and display explained variance ratios\n",
    "dataframe = df_transformed\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "x =dataframe.T\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, index = dataframe.T.index, columns = ['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10'])\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "sample_labels = 'ATTRIBUTE_collection_region'\n",
    "ref_for_labels = 'filename' #should be the column whose contents exactly match the sample names in df_transformed\n",
    "x_label = 'PC1 [9.7%]'\n",
    "y_label = 'PC2 [8.4%]'\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "samplenames = [dict_labels[n] for n in dataframe]\n",
    "samplenames\n",
    "\n",
    "principalDf['labels'] = samplenames\n",
    "\n",
    "groups = principalDf.groupby('labels')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05)\n",
    "for name, group in groups:\n",
    "    ax.plot(group.pc1, group.pc2, marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "ax.set_xlabel(x_label)\n",
    "ax.set_ylabel(y_label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature & Sample Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution showing frquency of features across samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, values = zip(*Counter(np.sum(df_transformed.T > 0)).most_common())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.title('Feature Frequency Distribution', fontsize=20)\n",
    "plt.ylabel('# of features', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('# of samples in which feature is detected', fontsize=18)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples, arranged by number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.sum(df_transformed > 0).sort_values(ascending = False).index\n",
    "values = np.sum(df_transformed > 0).sort_values(ascending = False).values\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.title('Features per sample', fontsize=20)\n",
    "plt.ylabel('# of features', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('samples', fontsize=18)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of sample-specific features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0).sort_values(ascending = False).index\n",
    "values = np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0).sort_values(ascending = False).values\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes, labels)\n",
    "plt.title('Sample-specific features per sample', fontsize=20)\n",
    "plt.ylabel('# of sample-specific feats.', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('samples', fontsize=18)\n",
    "plt.xticks(fontsize=10, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: Number of features vs max. peak area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = np.sum(df_transformed > 0)\n",
    "max_peaks = np.max(df_transformed)\n",
    "ax = pd.concat([max_peaks, num_feats], axis=1).plot.scatter(x=0,y=1)\n",
    "ax.set_ylabel('# of features')\n",
    "ax.set_xlabel('max. peak area [log10 transf.]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: Number of sample-specific features vs number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)\n",
    "\n",
    "num_feats = np.sum(df_transformed > 0)\n",
    "num_samp_spec = np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)\n",
    "ax = pd.concat([num_feats, num_samp_spec], axis=1).plot.scatter(x=0,y=1)\n",
    "ax.set_xlabel('# of features')\n",
    "ax.set_ylabel('# of sample-specific feats.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: Number of sample-specific features vs max peak area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)\n",
    "\n",
    "max_peaks = np.max(df_transformed)\n",
    "num_samp_spec = np.sum(df_transformed[np.sum(df_transformed.T > 0) == 1] > 0)\n",
    "ax = pd.concat([max_peaks, num_samp_spec], axis=1).plot.scatter(x=0,y=1)\n",
    "ax.set_ylabel('# of sample-specific feats.')\n",
    "ax.set_xlabel('max. peak area [log10 transf.]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering + Dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering + Dendrogram\n",
    "Hierarchical clustering will be performed on the bucket table generated via ORCA MS1 feature processing or on the bucket table that was independently loaded-in - whichever happened most recently.\n",
    "\n",
    "Here are the variables that must be set:\n",
    "\n",
    "**title**: The title that you would like to assign to the dendrogram.\n",
    "\n",
    "**sample_labels**: The name of the column from the metadata table that includes the names/identifiers of your samples that you would like to use in labelling the branches of the dendrogram.\n",
    "\n",
    "**ref_for_labels**: The name of the column for the metadata table that includes the names/identifiers of your samples as written in the bucket table. It is important that this variable is assigned correctly, as it allows metadata to be mapped properly to your samples.\n",
    "\n",
    "**metric**: The distance metric used in performing the hierarchical clustering. For more information on options for distance metrics, check out the documentation for SciPy, specifically the pdist function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\n",
    "\n",
    "**method**: The clustering method used in performing the hierarchical clustering. For more information on options for distance metrics, check out the documentation for SciPy, specifically the linkage function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n",
    "\n",
    "**color_cutoff**: The distance value below which you wish clusters to be colorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = 'Hierarchical Clustering of Induction Experiment Samples'\n",
    "sample_labels = 'ATTRIBUTE_collection_site'\n",
    "ref_for_labels = 'filename' #should be the column whose contents exactly match the sample names in df_transformed\n",
    "metric = 'cosine'\n",
    "method = 'average'\n",
    "color_cutoff = 0.5\n",
    "dataframe = df_transformed #usually df_transformed\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "samplenames = [dict_labels[n] for n in dataframe]\n",
    "\n",
    "#Clustering linkages followed by dendrogram construction\n",
    "matplotlib.rcParams['lines.linewidth'] = 3\n",
    "hc = linkage(dataframe.T, method, metric = metric)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(title, fontsize=20)\n",
    "plt.ylabel(metric + ' distance', fontsize=18)\n",
    "plt.yticks(fontsize=14)\n",
    "dendrogram(\n",
    "    hc,\n",
    "    color_threshold=color_cutoff,\n",
    "    above_threshold_color='k',\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=14,\n",
    "    labels = samplenames   \n",
    ")\n",
    "#set_link_color_palette(['blue', 'red'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cophenetic correlation\n",
    "The cophenetic correlation coefficient is a measure of how well the linkages established in hierarchical clustering represent the actual calculated distances between pairs of samples. We can use it to judge how well a dendrogram represents to the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining how well the dendrogram represents the cosine distances in the data\n",
    "ccorr, distances = cophenet(hc, pdist(dataframe.T, metric))\n",
    "ccorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering - tabular output\n",
    "In some instances, it may be useful to have a tabular output of the hierarchical clustering that is visualized via dendrogram above. Generate such a table by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table output of hierarchical clustering\n",
    "hc_df = pd.DataFrame(hc)\n",
    "hc_df.columns = ['cluster 1', 'cluster 2', 'distance at merge', 'number of samples in new cluster']\n",
    "hc_df['new cluster number'] = [c for c in range(len(list(df_transformed.T.index)),len(list(df_transformed.T.index))+len(hc_df))]\n",
    "\n",
    "c1_alias = []\n",
    "for c in hc_df['cluster 1']:\n",
    "    if c < len(list(df_transformed.T.index)):\n",
    "        c1_alias.append(list(df_transformed.T.index)[int(c)])\n",
    "    else:\n",
    "        c1_alias.append('none')\n",
    "        \n",
    "c2_alias = []\n",
    "for c in hc_df['cluster 2']:\n",
    "    if c < len(list(df_transformed.T.index)):\n",
    "        c2_alias.append(list(df_transformed.T.index)[int(c)])\n",
    "    else:\n",
    "        c2_alias.append('none')\n",
    "\n",
    "\n",
    "hc_df['cluster 1 alias'] = c1_alias\n",
    "hc_df['cluster 2 alias'] = c2_alias\n",
    "hc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise distance matrix\n",
    "In some instances, it may also be useful to generate a pairwise distance matrix for the samples in the bucket table. Do so by running the cell below. Note: This will use the same distance metric as specified above in the hierarchical clustering parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise = pd.DataFrame(squareform(pdist(df_transformed.T, metric)))\n",
    "pairwise.index = df_transformed.T.index\n",
    "pairwise.columns = df_transformed.T.index\n",
    "pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating core and specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for group-specific features that do not occur elsewhere in dataset\n",
    "These features occur in all samples from group, but not in other samples\n",
    "\n",
    "**category**: metadata column with labels for group of interest\n",
    "\n",
    "**interest**: label for group of interest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for presence in selected group vs absences in rest [length of regionally-specific core feats]\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "interest = 'Saipan'\n",
    "\n",
    "####################################################################\n",
    "\n",
    "files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == interest]['filename'].values]\n",
    "\n",
    "empty_feats = df_transformed[np.max(df_transformed.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "core_metab = df_transformed[np.min(df_transformed[files_of_interest].T) > 0].index\n",
    "\n",
    "query_result = [r for r in core_metab if r in empty_feats]\n",
    "print(len(query_result))\n",
    "query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for group-specific features that do not occur elsewhere in dataset [from limited dataset]\n",
    "These features occur in all samples from group, but not in other samples\n",
    "\n",
    "**limit_category**: metadata column that includes labels for group that you would like to limit the dataset to\n",
    "\n",
    "**limit**: group label that you would like to limit the dataset to\n",
    "\n",
    "**category**: metadata column with labels for group of interest\n",
    "\n",
    "**interest**: label for group of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for presence in selected group vs absences in rest\n",
    "limit_category = 'ATTRIBUTE_collection_region'\n",
    "limit = 'Guam'\n",
    "\n",
    "category = 'ATTRIBUTE_collection_site'\n",
    "interest = 'Finger Reef'\n",
    "\n",
    "####################################################################\n",
    "file_limit = [c for c in df_transformed.columns if c in md[md[limit_category] == limit]['filename'].values]\n",
    "limited = df_transformed[file_limit]\n",
    "\n",
    "files_of_interest = [c for c in limited.columns if c in md[md[category] == interest]['filename'].values]\n",
    "\n",
    "df_transformed[np.max(limited.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "df_transformed[np.min(limited[files_of_interest].T) > 0].index\n",
    "\n",
    "query_result = [r for r in limited[np.max(limited.drop(files_of_interest, axis=1).T) <= 0].index if r in limited[np.min(limited[files_of_interest].T) > 0].index]\n",
    "print(len(query_result))\n",
    "query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate table of feature, core, and specific core counts, per group\n",
    "\n",
    "**category**: metadata column including labels of groups of interest\n",
    "\n",
    "**cutoff**: proportion of samples per group that must contain feature, in order for it to be counted as core\n",
    "\n",
    "**dataframe**: source dataframe to draw from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate table with core, specific core, avg + sd of features per sample, # of samples; PER CATEGORY\n",
    "\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "dataframe = df_transformed\n",
    "\n",
    "#############################################################################\n",
    "core_summary_df = pd.DataFrame()\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    files_of_interest = [c for c in dataframe.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    feats = [np.sum(dataframe[f] > 0) for f in files_of_interest]\n",
    "    empty_feats = dataframe[np.max(dataframe.drop(files_of_interest, axis=1).T) <= 0].index\n",
    "    core_metab = dataframe[np.sum(dataframe[files_of_interest].T > 0)/len(files_of_interest) >= cutoff].index\n",
    "    query_result = [r for r in core_metab if r in empty_feats]\n",
    "\n",
    "    core_summary_df[cat] = [np.mean(feats), np.std(feats), len(core_metab), len(query_result), len(files_of_interest)]\n",
    "    \n",
    "core_summary_df.drop(['na','unknown','Panama', 'Puerto Rico'], axis=1, inplace=True)\n",
    "core_summary_df = core_summary_df.sort_index(axis=1)\n",
    "core_summary_df.index = ['mean - feats/sample', 'std - feats/sample', 'core feats', 'specific core feats', 'sample size']\n",
    "core_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export table as csv\n",
    "\n",
    "**path**: file path and name for the export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cores_summary_table_075.csv'\n",
    "#######################################################\n",
    "core_summary_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble dataframe only including features that are core to at least one group\n",
    "\n",
    "**category**: metadata column including labels of groups of interest\n",
    "\n",
    "**cutoff**: proportion of samples per group that must have peaks of a feature in order for it to be counted as core\n",
    "\n",
    "**dataframe**: source dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble dataframe of cores [all samples]\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "dataframe = df_transformed #usually df_transformed\n",
    "\n",
    "#################################################################\n",
    "all_core = []\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    files_of_interest = [c for c in dataframe.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    core_metab = dataframe[np.sum(dataframe[files_of_interest].T > 0)/len(files_of_interest) >= cutoff].index\n",
    "    all_core.extend(core_metab)\n",
    "    \n",
    "all_core = list(set(all_core))\n",
    "\n",
    "cores = dataframe.loc[all_core]\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataframe indicating which features are core to which groups\n",
    "\n",
    "***NOTE***: This is needed for generating pairwise table of overlapping core feats\n",
    "    \n",
    "**category**: metadata column including labels of groups of interest\n",
    "\n",
    "**cutoff**: proportion of samples in a group that must have feature peak in order for that feature to count as core\n",
    "\n",
    "**groups_to_drop**: list of groups to drop, that you would like to exclude from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble dataframe of cores [by group]\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 0.75\n",
    "cats_to_drop = ['na','unknown','Panama', 'Puerto Rico']\n",
    "\n",
    "#################################################################\n",
    "all_core_df = pd.DataFrame()\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    all_core_df[cat] = np.sum(df_transformed[files_of_interest].T > 0)/len(files_of_interest) >= cutoff\n",
    "\n",
    "all_core_df.drop(groups_to_drop, axis=1, inplace=True)\n",
    "#all_core_df = all_core_df[np.max(all_core_df.T) >= cutoff]\n",
    "all_core_df = all_core_df[np.sum(all_core_df.T) > 0]\n",
    "all_core_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise table of core features overlapping between groups\n",
    "\n",
    "**count_v_prop**: display counts of overlapping core features 'count' or display proportion of overlapping core features 'prop'\n",
    "\n",
    "**output**: options for how to present output: 'table' to print datatable, 'export' to produce csv, or 'hetamap' to produce a heatmap\n",
    "\n",
    "**path**: file path and name for outputted csv, if 'export' is selected for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_v_prop = 'count'\n",
    "output = 'table'\n",
    "path = './output.csv'\n",
    "\n",
    "##############################################################################################\n",
    "hm_cores = pd.DataFrame(index = all_core_df.columns, columns = all_core_df.columns)\n",
    "for col1 in all_core_df.columns:\n",
    "    for col2 in all_core_df.columns:\n",
    "        shared_core = len([m for m in all_core_df[all_core_df[col1]].index if m in all_core_df[all_core_df[col2]].index])\n",
    "        hm_cores.at[col1, col2] = shared_core\n",
    "\n",
    "hm_cores = hm_cores.sort_index(axis=0)        \n",
    "hm_cores = hm_cores.sort_index(axis=1)\n",
    "hm_cores = hm_cores.astype(float)\n",
    "hm_cores_prop = hm_cores/np.max(hm_cores)\n",
    "hm_cores_prop = hm_cores_prop.astype(float)\n",
    "\n",
    "if count_v_prop == 'count':\n",
    "    output_df = hm_cores\n",
    "elif count_v_prop == 'prop':\n",
    "    output_df = hm_cores_prop\n",
    "else:\n",
    "    print('Error - please set count_v_prop to count or prop.')\n",
    "if output == 'table':\n",
    "    display(output_df)\n",
    "elif output == 'export':\n",
    "    output_df.to_csv(path)\n",
    "elif output == 'heatmap':\n",
    "    sns.heatmap(output_df, annot=False)\n",
    "else:\n",
    "    print('Error - please select table, export, or heatmap for output type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rarefation and core-size tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate rarefaction curve\n",
    "\n",
    "**category**: name of metadata column that includes labels for th groups that you would like to have rarefaction curves generated for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rarefaction\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "\n",
    "#################################################################\n",
    "rarefaction = pd.DataFrame()\n",
    "labels = []\n",
    "samples = []\n",
    "feats = []\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    print(cat)\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    comps = []\n",
    "    for n in range(1, len(files_of_interest)+1):\n",
    "        comps.extend(list(itertools.combinations(files_of_interest, n)))\n",
    "\n",
    "    for comp in comps:\n",
    "        labels.append(cat) \n",
    "        samples.append(len(comp))\n",
    "        feats.append(np.sum(np.sum(df_transformed[list(comp)].T > 0) > 0))\n",
    "        \n",
    "rarefaction['labels'] = labels\n",
    "rarefaction['samples'] = samples\n",
    "rarefaction['features'] = feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rarefaction curve\n",
    "\n",
    "**zoom**: Set as 'True', for a zoomed in view of the rarefaction curve\n",
    "\n",
    "**y_min**: If 'zoom' is set to 'True', this will set the minimum value for the y-axis\n",
    "\n",
    "**y_max**: If 'zoom' is set to 'True', this will set the maximum value for the y-axis\n",
    "\n",
    "**x_min**: If 'zoom' is set to 'True', this will set the minimum value for the x-axis\n",
    "\n",
    "**x_max**: If 'zoom' is set to 'True', this will set the maximum value for the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rarefaction curve\n",
    "zoom = False\n",
    "y_min = 0\n",
    "y_max = 800\n",
    "X_min = 1\n",
    "x_max = 5\n",
    "\n",
    "########################################################################\n",
    "\n",
    "ax = sns.lineplot(x=\"samples\", y=\"features\",\n",
    "             hue=\"labels\",\n",
    "             data=rarefaction, estimator='mean', ci='sd')\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "if zoom == True:\n",
    "    ax.set_ylim(y_min,y_max)\n",
    "    ax.set_xlim(x_min,x_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate curve, showing change in number of core features as sampling increases\n",
    "\n",
    "**category**: name of metadata column that includes labels for th groups that you would like to have rarefaction curves generated for\n",
    "\n",
    "**cutoff**: proportion of samples per group that must contain feature in order for it to be counted as 'core'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core-size vs sampling\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "\n",
    "#################################################################\n",
    "core_v_samp = pd.DataFrame()\n",
    "labels = []\n",
    "samples = []\n",
    "feats = []\n",
    "\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    print(cat)\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    comps = []\n",
    "    for n in range(1, len(files_of_interest)+1):\n",
    "        comps.extend(list(itertools.combinations(files_of_interest, n)))\n",
    "\n",
    "    for comp in comps:\n",
    "        labels.append(cat) \n",
    "        samples.append(len(comp))\n",
    "        feats.append(np.sum(np.sum(df_transformed[list(comp)].T > 0)/len(comp) >= cutoff))\n",
    "        \n",
    "core_v_samp['labels'] = labels\n",
    "core_v_samp['samples'] = samples\n",
    "core_v_samp['features'] = feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot core-sampling curve\n",
    "\n",
    "**zoom**: Set as 'True', for a zoomed in view of the rarefaction curve\n",
    "\n",
    "**y_min**: If 'zoom' is set to 'True', this will set the minimum value for the y-axis\n",
    "\n",
    "**y_max**: If 'zoom' is set to 'True', this will set the maximum value for the y-axis\n",
    "\n",
    "**x_min**: If 'zoom' is set to 'True', this will set the minimum value for the x-axis\n",
    "\n",
    "**x_max**: If 'zoom' is set to 'True', this will set the maximum value for the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot core-size per sample size curve\n",
    "zoom = True\n",
    "y_min = 0\n",
    "y_max = 800\n",
    "X_min = 1\n",
    "x_max = 5\n",
    "\n",
    "########################################################################\n",
    "\n",
    "ax = sns.lineplot(x=\"samples\", y=\"features\", hue=\"labels\",\n",
    "             data=core_v_samp, estimator='mean', ci='sd')\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "if zoom == True:\n",
    "    ax.set_ylim(y_min,y_max)\n",
    "    ax.set_xlim(x_min,x_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate curve, showing change in number of core features as sampling increases, scaled to mean number of features\n",
    "\n",
    "**category**: name of metadata column that includes labels for th groups that you would like to have rarefaction curves generated for\n",
    "\n",
    "**cutoff**: proportion of samples per group that must contain feature in order for it to be counted as 'core'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core per mean samps\n",
    "category = 'ATTRIBUTE_collection_region_GuamShrimp'\n",
    "cutoff = 1\n",
    "\n",
    "#################################################################\n",
    "core_v_samp = pd.DataFrame()\n",
    "labels = []\n",
    "samples = []\n",
    "feats = []\n",
    "core_vals = []\n",
    "props = []\n",
    "\n",
    "for cat in md[category].unique():\n",
    "    print(cat)\n",
    "    files_of_interest = [c for c in df_transformed.columns if c in md[md[category] == cat]['filename'].values]\n",
    "    comps = []\n",
    "    for n in range(1, len(files_of_interest)+1):\n",
    "        comps.extend(list(itertools.combinations(files_of_interest, n)))\n",
    "\n",
    "    for comp in comps:\n",
    "        labels.append(cat) \n",
    "        samples.append(len(comp))\n",
    "        feats.append(np.sum(np.sum(df_transformed[list(comp)].T > 0) > 0))\n",
    "        core_vals.append(np.sum(np.sum(df_transformed[list(comp)].T > 0)/len(comp) >= cutoff))\n",
    "        props.append(np.sum(np.sum(df_transformed[list(comp)].T > 0)/len(comp) >= cutoff)/np.sum(np.sum(df_transformed[list(comp)].T > 0) > 0))\n",
    "        \n",
    "core_v_samp['labels'] = labels\n",
    "core_v_samp['samples'] = samples\n",
    "core_v_samp['features'] = feats\n",
    "core_v_samp['core_va']\n",
    "core_v_samp['proportion'] = props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot core-sampling feature-scaled curve\n",
    "\n",
    "**zoom**: Set as 'True', for a zoomed in view of the rarefaction curve\n",
    "\n",
    "**y_min**: If 'zoom' is set to 'True', this will set the minimum value for the y-axis\n",
    "\n",
    "**y_max**: If 'zoom' is set to 'True', this will set the maximum value for the y-axis\n",
    "\n",
    "**x_min**: If 'zoom' is set to 'True', this will set the minimum value for the x-axis\n",
    "\n",
    "**x_max**: If 'zoom' is set to 'True', this will set the maximum value for the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "zoom = False\n",
    "y_min = 0\n",
    "y_max = 800\n",
    "X_min = 1\n",
    "x_max = 5\n",
    "\n",
    "###################################################################################\n",
    "ax = sns.lineplot(x=\"samples\", y=\"proportion\", hue=\"labels\",\n",
    "             data=core_v_samp, estimator='mean', ci='sd')\n",
    "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
    "if zoom == True:\n",
    "    ax.set_ylim(y_min,y_max)\n",
    "    ax.set_xlim(x_min,x_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for within group statistically significant differences\n",
    "This code generates pairwise values that indicate how related each pair of samples is: cosine distance, number of shared features, proportion of shared features (based on the smaller feature count of each pair). These pairwise scores are labelled based on whether the two samples cam from the same group, or different groups. Then, a t-test is used to test the null hypothesis that mean score (cosine distance, number of shared features, proportion of shared features) for pairs within the same groups is not statistically significant for the mean score of pairs from separate groups.\n",
    "\n",
    "The cell below does the pairwise calculations.\n",
    "\n",
    "**category**: metadata column indicating the groups that you wish to test\n",
    "\n",
    "**dataframe**: the name of the dataframe to pull data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pairwise values for cosine distance and number of shared features (prop?)\n",
    "category = 'ATTRIBUTE_shrimp_v_nonshrimp'\n",
    "dataframe = limited\n",
    "\n",
    "########################################################\n",
    "same_group = []\n",
    "cos_dist = []\n",
    "shared_feats = []\n",
    "sf_prop = []\n",
    "\n",
    "for pair in list(itertools.combinations(dataframe.columns, 2)):\n",
    "    same_group.append(md[md['filename'] == pair[0]][category].values[0] == md[md['filename'] == pair[1]][category].values[0])\n",
    "    cos_dist.append(cosine(dataframe[pair[0]].values, dataframe[pair[1]].values))\n",
    "    shared_feats.append(np.sum(np.sum(dataframe[[pair[0], pair[1]]].T > 0) == 2))\n",
    "    sf_prop.append(np.sum(np.sum(dataframe[[pair[0], pair[1]]].T > 0) == 2)/np.min(np.sum(dataframe[[pair[0], pair[1]]] > 0).values))\n",
    "\n",
    "stats_test = pd.DataFrame(index = same_group)\n",
    "stats_test['cos_dist'] = cos_dist\n",
    "stats_test['shared_feats'] = shared_feats\n",
    "stats_test['sf_prop'] = sf_prop\n",
    "\n",
    "stats_test\n",
    "                                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test of pairwise calculated scores\n",
    "\n",
    "**measure**:'cos_dist', 'shared_feats', or 'sf_prop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test to test if distributions of cosine distance and shared core features differ\\\n",
    "## between two populations (same label vs different label)\n",
    "measure = 'sf_prop'\n",
    "###############################################################################################\n",
    "ttest_ind(stats_test.loc[True][measure], stats_test.loc[False][measure], equal_var=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection + Heat Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "For feature selection, ORCA uses univariate feature selection, as implemented in sklearn (https://scikit-learn.org/stable/). Univariate feature selection was selected as the optimal strategy of feature selection for this particular problem (e.g. identifying the most important features in driving clustering of different groups of samples) as it considers features individually and so is not negatively impacted by correlated features. In order to truly judge the statistical significance of features, univariate feature selection must be applied to a dataset with proper numbers of replicates per category of sample, and must also meet the three assumptions necessary for applying the ANOVA statistical test (here is a good resource for reviewing those assumptions: https://sites.ualberta.ca/~lkgray/uploads/7/3/6/2/7362679/slides_-_anova_assumptions.pdf) Even in cased where the assumptions are not met, or there are not enough replicate samples per category, feature selection can still be helpful for generating hypotheses about which features may be significant. To conduct univariate feature selection, please set the below parameters:\n",
    "\n",
    "**sample_labels**: Input the column name from the metadata table that you would like samples to be grouped by.\n",
    "\n",
    "**ref_for_labels**: This should be the column name whose contents exactly match the sample names in df_transformed. This will allow the sample (grouping) labels to be properly mapped to the samples.\n",
    "\n",
    "**top_k**: Number of top features to display.\n",
    "\n",
    "**rank_by**: Column to rank results by. This could be 'p-value' or 'F-values', if the goal is to see the most statistically significant features, as determined by univariate feature selection. \n",
    "\n",
    "**ascending**: Setting this variable to 'True' will list values from the designated column (as indicated with the 'rank_by' variable) from smallest to largest, while setting it to 'False' will list the values from largest to smallest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels = 'location'\n",
    "ref_for_labels = 'filename_path' #should be the column whose contents exactly match the sample names in df_transformed\n",
    "\n",
    "top_k = 5\n",
    "rank_by = 'Saipan'\n",
    "ascending = False\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "samplenames = [dict_labels[n] for n in df_transformed]\n",
    "\n",
    "df_featselect = df_transformed.copy(deep = True)\n",
    "df_featselect.index = list(df_transformed.index)\n",
    "\n",
    "F_values, p_values = sklearn.feature_selection.f_classif(df_featselect.T.values,samplenames)\n",
    "feat_select_results = pd.DataFrame(index = list(df_transformed.index))\n",
    "feat_select_results['F-values'] = F_values\n",
    "feat_select_results['p-values'] = p_values\n",
    "\n",
    "group_means = df_featselect.T.groupby(md['location'].values).agg('mean').T\n",
    "\n",
    "fs = pd.concat([feat_select_results, group_means], axis = 1).sort_values(by = rank_by, ascending=ascending).head(top_k)\n",
    "fs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map visualizations\n",
    "Heat maps can be generated for the feature selection output, or for an excerpt of the features vs samples bucket table.\n",
    "\n",
    "**feat_select_or_bucket_table**: Indicate either 'feat_select' or 'bucket_table', depending on which you would like a heat map generated of.\n",
    "\n",
    "If you select 'bucket_table', you can also indicate an m/z range for which features you would like included.\n",
    "\n",
    "**mz_high**: Upper limit of m/z for inclusion in the heat map\n",
    "\n",
    "**mz_low**: Lower limit of m/z for inclusion in the heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_select_or_bucket_table = 'bucket_table'\n",
    "mz_high = 460\n",
    "mz_low = 450\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "if feat_select_or_bucket_table == 'feat_select':\n",
    "    ax = sns.heatmap(fs.drop(['p-values', 'F-values'], axis=1))\n",
    "elif feat_select_or_bucket_table == 'bucket_table':\n",
    "    ax = sns.heatmap(df_transformed.query('mz > @mz_low and mz < @mz_high'))\n",
    "else:\n",
    "    print('invalid input. Try \"feat_select\" or \"bucket_table\".')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentative Identification of Top Features\n",
    "\n",
    "***NOTE***: This code currently only works for buckettables generated via ORCA MS1 feature processing\n",
    "\n",
    "While m/z of an MS1 feature is not enough information to confidently identify what compound is in a sample, cross referencing features with lists of known compounds can allow for quick dereplication, which is very helfpul in the context of natural products discovery. The cell below allows one to load in a table of known compounds, their masses, and expected m/z for protenated and sodiated peaks, and then annotate features in your dataset based on this set of known compounds. By default, features are ordered based on which are the most prominent across the entire dataset (e.g. which features have the largest values).\n",
    "\n",
    "The parameters to set are:\n",
    "\n",
    "**mz_tolerance**: How close a feature's m/z must be to the protenated or sodiated m/z of a known compound in order for the feature to be annotated as that known compound.\n",
    "\n",
    "**top_k**: Number of features to output. Note: features are ranked by greattest value to smallest value, as determined by selecting the max value for a feature across all samples.\n",
    "\n",
    "**db_path**: Path to table of known compounds to be used for annotation. Table must be a csv file with at least three columns: 'Name', 'Protenated Peak', and 'Sodiated Peak'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_tolerance = 1\n",
    "top_k = 10\n",
    "db_path = './Moorea_bouillonii_db.csv'\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "\n",
    "db = pd.read_csv(db_path)\n",
    "\n",
    "df_top = df_transformed.T.max().sort_values(ascending=False).head(top_k).reset_index()\n",
    "\n",
    "df_top.columns = ['mz', 'rt', 'max transformed integral']\n",
    "\n",
    "column_pids = []\n",
    "column_difs = []\n",
    "for m in df_top['mz']:\n",
    "    putative_ids = []\n",
    "    difs = []\n",
    "    for i in db.index:\n",
    "        if abs(m - db.iloc[i]['Protenated Peak']) <= mz_tolerance:\n",
    "            putative_ids.append(db.iloc[i]['Name'] + ' [M+H]+')\n",
    "            difs.append(round(abs(m - db.iloc[i]['Protenated Peak']),2))\n",
    "    for i in db.index:\n",
    "        if abs(m - db.iloc[i]['Sodiated Peak']) <= mz_tolerance:\n",
    "            putative_ids.append(db.iloc[i]['Name'] + ' [M+Na]+')\n",
    "            difs.append(round(abs(m - db.iloc[i]['Sodiated Peak']),2))\n",
    "    if len(putative_ids) == 0:\n",
    "        putative_ids.append('None')\n",
    "        difs.append(0)\n",
    "    column_pids.append(putative_ids)\n",
    "    column_difs.append(difs)\n",
    "    \n",
    "df_top['putative ids'] = column_pids\n",
    "df_top['difference'] = column_difs\n",
    "df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Feature Priority List\n",
    "Create consolidated list of top k features in each sample, and query against user-supplied datasheet of known compounds. Queries based on *m/z*.\n",
    "\n",
    "**k**: number of top features to consider, per sample\n",
    "\n",
    "**mz_tolerance**: Maximum number of Daltons difference between a feature and known compound compared against, in order for that featrue to be putatively identified as that known compound.\n",
    "\n",
    "**db_path**: path to user-supplied known compound datasheet (csv format) to compare against. Table must be a csv file with at least three columns: 'Name', 'Protenated Peak', and 'Sodiated Peak'.\n",
    "\n",
    "**sample_labels**: metadata column to use to label what sample groups a particular top feature came from.\n",
    "\n",
    "**ref_for_labels** metadata column of sample names. This column will be used to map metadata to samples included in the buckettable of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate top peaks, show which locations from, and do putative ids\n",
    "k = 5\n",
    "mz_tolerance = 0.1\n",
    "db_path = './data/Moorea_bouillonii_db.csv'\n",
    "sample_labels = 'ATTRIBUTE_collection_region'\n",
    "ref_for_labels = 'filename'\n",
    "#######################################################################################################\n",
    "\n",
    "indices = []\n",
    "samples = []\n",
    "dict_labels = pd.Series(md[sample_labels].values, index=md[ref_for_labels]).to_dict()\n",
    "\n",
    "for samp in df_transformed.columns:\n",
    "    indices.extend(list(df_transformed[samp].sort_values(ascending=False).head(k).index))\n",
    "    samples.extend([samp] * k)\n",
    "\n",
    "df = pd.DataFrame([indices,samples]).T.sort_values(by=0)\n",
    "gb = df.groupby(0)[1].unique()\n",
    "df = pd.concat([col_ref_df, gb], axis=1, join = 'inner')\n",
    "df.drop(['Unnamed: 159', 'row ID'], axis=1, inplace=True)\n",
    "df.rename(columns={'row m/z': 'm/z', 'row retention time':'retention time', 1: \"samples\"}, inplace = True)\n",
    "locs = []\n",
    "for s in df['samples']:\n",
    "    samplenames = [dict_labels[ss] for ss in s]\n",
    "    locs.append(list(set(samplenames)))\n",
    "df['source location'] = locs\n",
    "\n",
    "db = pd.read_csv(db_path)\n",
    "\n",
    "column_pids = []\n",
    "column_difs = []\n",
    "for m in df['m/z']:\n",
    "    putative_ids = []\n",
    "    difs = []\n",
    "    for i in db.index:\n",
    "        if abs(m - db.iloc[i]['Protenated Peak']) <= mz_tolerance:\n",
    "            putative_ids.append(db.iloc[i]['Name'] + ' [M+H]+')\n",
    "            difs.append(round(abs(m - db.iloc[i]['Protenated Peak']),2))\n",
    "    for i in db.index:\n",
    "        if abs(m - db.iloc[i]['Sodiated Peak']) <= mz_tolerance:\n",
    "            putative_ids.append(db.iloc[i]['Name'] + ' [M+Na]+')\n",
    "            difs.append(round(abs(m - db.iloc[i]['Sodiated Peak']),2))\n",
    "    if len(putative_ids) == 0:\n",
    "        putative_ids.append('None')\n",
    "        difs.append(0)\n",
    "    column_pids.append(putative_ids)\n",
    "    column_difs.append(difs)\n",
    "    \n",
    "df['putative ids'] = column_pids\n",
    "df['difference'] = column_difs\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export priority list table as a csv\n",
    "**path**: path and name for output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './priority_list.csv'\n",
    "##################################################################\n",
    "df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous \n",
    "Here are some bits of code that might be useful. Feel free to add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of features per sample, post transformations\n",
    "df_transformed[df_transformed > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many samples different features occur in\n",
    "np.sum(df_transformed.T > 0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features that occur in k samples\n",
    "k = 1\n",
    "##########################################################\n",
    "df_transformed[np.sum(df_transformed.T > 0) == k].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up m/z and rt for a specific feature\n",
    "feat_num = 49\n",
    "####################################################\n",
    "col_ref_df.loc[feat_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to see if feat (esp. singletons) was aligned optimally\n",
    "## via compare it to other feats with similar m/z\n",
    "\n",
    "min_mz = 457\n",
    "max_mz = 458\n",
    "#######################################\n",
    "\n",
    "col_ref_df[col_ref_df['row m/z'] > min_mz][col_ref_df[col_ref_df['row m/z'] > min_mz]['row m/z'] < max_mz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export a dataframe to csv\n",
    "df_transformed.to_csv('bucket_table-transformed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
